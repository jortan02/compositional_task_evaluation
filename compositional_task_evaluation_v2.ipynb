{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at google/flan-t5-base and are newly initialized: ['decoder.embed_tokens.weight', 'encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "c:\\Users\\Jordan\\anaconda3\\envs\\compositional\\Lib\\site-packages\\transformers\\generation\\utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Evaluation module cache file doesn't exist. Please make sure that you call `add` or `add_batch` at least once before calling `compute`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39m\"\"\"Evaluate accuracy\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39meval\u001b[39m \u001b[39m=\u001b[39m evaluator(\u001b[39m\"\u001b[39m\u001b[39mtext-generation\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m result \u001b[39m=\u001b[39m \u001b[39meval\u001b[39;49m\u001b[39m.\u001b[39;49mcompute(\n\u001b[0;32m     21\u001b[0m     model_or_pipeline\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m     22\u001b[0m     data\u001b[39m=\u001b[39;49mtest_dataset,\n\u001b[0;32m     23\u001b[0m     metric\u001b[39m=\u001b[39;49maccuracy,\n\u001b[0;32m     24\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[0;32m     25\u001b[0m     input_column\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     26\u001b[0m     label_column\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39manswer\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     28\u001b[0m \u001b[39mprint\u001b[39m(result)\n",
      "File \u001b[1;32mc:\\Users\\Jordan\\anaconda3\\envs\\compositional\\Lib\\site-packages\\evaluate\\evaluator\\base.py:261\u001b[0m, in \u001b[0;36mEvaluator.compute\u001b[1;34m(self, model_or_pipeline, data, subset, split, metric, tokenizer, feature_extractor, strategy, confidence_level, n_resamples, device, random_state, input_column, label_column, label_mapping)\u001b[0m\n\u001b[0;32m    258\u001b[0m metric_inputs\u001b[39m.\u001b[39mupdate(predictions)\n\u001b[0;32m    260\u001b[0m \u001b[39m# Compute metrics from references and predictions\u001b[39;00m\n\u001b[1;32m--> 261\u001b[0m metric_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metric(\n\u001b[0;32m    262\u001b[0m     metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[0;32m    263\u001b[0m     metric_inputs\u001b[39m=\u001b[39;49mmetric_inputs,\n\u001b[0;32m    264\u001b[0m     strategy\u001b[39m=\u001b[39;49mstrategy,\n\u001b[0;32m    265\u001b[0m     confidence_level\u001b[39m=\u001b[39;49mconfidence_level,\n\u001b[0;32m    266\u001b[0m     n_resamples\u001b[39m=\u001b[39;49mn_resamples,\n\u001b[0;32m    267\u001b[0m     random_state\u001b[39m=\u001b[39;49mrandom_state,\n\u001b[0;32m    268\u001b[0m )\n\u001b[0;32m    270\u001b[0m \u001b[39m# TODO: To clarify why `wer` and `cer` return float\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[39m# even though metric.compute contract says that it\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[39m# returns Optional[dict].\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(metric_results) \u001b[39m==\u001b[39m \u001b[39mfloat\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Jordan\\anaconda3\\envs\\compositional\\Lib\\site-packages\\evaluate\\evaluator\\base.py:467\u001b[0m, in \u001b[0;36mEvaluator.compute_metric\u001b[1;34m(self, metric, metric_inputs, strategy, confidence_level, n_resamples, random_state)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_metric\u001b[39m(\n\u001b[0;32m    458\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    459\u001b[0m     metric: EvaluationModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    464\u001b[0m     random_state: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    465\u001b[0m ):\n\u001b[0;32m    466\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute and return metrics.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 467\u001b[0m     result \u001b[39m=\u001b[39m metric\u001b[39m.\u001b[39;49mcompute(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmetric_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mMETRIC_KWARGS)\n\u001b[0;32m    469\u001b[0m     \u001b[39mif\u001b[39;00m strategy \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbootstrap\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    470\u001b[0m         metric_keys \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mkeys()\n",
      "File \u001b[1;32mc:\\Users\\Jordan\\anaconda3\\envs\\compositional\\Lib\\site-packages\\evaluate\\module.py:433\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(v \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    432\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_batch(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[1;32m--> 433\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_finalize()\n\u001b[0;32m    435\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_file_name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilelock \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jordan\\anaconda3\\envs\\compositional\\Lib\\site-packages\\evaluate\\module.py:385\u001b[0m, in \u001b[0;36mEvaluationModule._finalize\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m Dataset\u001b[39m.\u001b[39mfrom_buffer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_writer\u001b[39m.\u001b[39mgetvalue())\n\u001b[0;32m    383\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_id \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    384\u001b[0m     \u001b[39m# Let's acquire a lock on each node files to be sure they are finished writing\u001b[39;00m\n\u001b[1;32m--> 385\u001b[0m     file_paths, filelocks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_all_cache_files()\n\u001b[0;32m    387\u001b[0m     \u001b[39m# Read the predictions and references\u001b[39;00m\n\u001b[0;32m    388\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Jordan\\anaconda3\\envs\\compositional\\Lib\\site-packages\\evaluate\\module.py:302\u001b[0m, in \u001b[0;36mEvaluationModule._get_all_cache_files\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_process \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    301\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_file_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 302\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    303\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mEvaluation module cache file doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt exist. Please make sure that you call `add` or `add_batch` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    304\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mat least once before calling `compute`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    305\u001b[0m         )\n\u001b[0;32m    306\u001b[0m     file_paths \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_file_name]\n\u001b[0;32m    307\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Evaluation module cache file doesn't exist. Please make sure that you call `add` or `add_batch` at least once before calling `compute`."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from datasets import Dataset\n",
    "from evaluate import evaluator\n",
    "import evaluate\n",
    "\n",
    "\"\"\"Define model\"\"\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", device_map=\"auto\")\n",
    "\"\"\"Define tokenizer\"\"\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\"\"\" Define dataset\"\"\"\n",
    "test_dataset = Dataset.from_pandas(pd.read_csv(\"multiply.csv\")).select(range(10))\n",
    "\"\"\" Define evaluator\"\"\"\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "# accuracy.add(reference=test_dataset.features[\"label\"])\n",
    "\"\"\"Evaluate accuracy\"\"\"\n",
    "eval = evaluator(\"text-generation\")\n",
    "\n",
    "result = eval.compute(\n",
    "    model_or_pipeline=model,\n",
    "    data=test_dataset,\n",
    "    metric=accuracy,\n",
    "    tokenizer=tokenizer,\n",
    "    input_column=\"question\",\n",
    "    label_column=\"answer\"\n",
    ")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
