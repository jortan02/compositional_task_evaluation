mkdir: cannot create directory ‘/scratch/general/vast/u1283221/huggingface_cache’: File exists
Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]Downloading spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 129MB/s]
Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]Downloading (…)cial_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 14.0MB/s]
Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]Downloading (…)okenizer_config.json: 100%|██████████| 2.54k/2.54k [00:00<00:00, 15.4MB/s]
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Downloading (…)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|██████████| 1.40k/1.40k [00:00<00:00, 12.2MB/s]
Downloading pytorch_model.bin:   0%|          | 0.00/308M [00:00<?, ?B/s]Downloading pytorch_model.bin:   3%|▎         | 10.5M/308M [00:00<00:05, 55.2MB/s]Downloading pytorch_model.bin:   7%|▋         | 21.0M/308M [00:00<00:04, 57.7MB/s]Downloading pytorch_model.bin:  10%|█         | 31.5M/308M [00:00<00:04, 64.1MB/s]Downloading pytorch_model.bin:  14%|█▎        | 41.9M/308M [00:00<00:03, 74.0MB/s]Downloading pytorch_model.bin:  17%|█▋        | 52.4M/308M [00:00<00:03, 82.4MB/s]Downloading pytorch_model.bin:  24%|██▍       | 73.4M/308M [00:00<00:02, 97.7MB/s]Downloading pytorch_model.bin:  31%|███       | 94.4M/308M [00:01<00:02, 102MB/s] Downloading pytorch_model.bin:  37%|███▋      | 115M/308M [00:01<00:01, 114MB/s] Downloading pytorch_model.bin:  44%|████▍     | 136M/308M [00:01<00:01, 123MB/s]Downloading pytorch_model.bin:  51%|█████     | 157M/308M [00:01<00:01, 129MB/s]Downloading pytorch_model.bin:  58%|█████▊    | 178M/308M [00:01<00:00, 133MB/s]Downloading pytorch_model.bin:  65%|██████▍   | 199M/308M [00:01<00:00, 136MB/s]Downloading pytorch_model.bin:  72%|███████▏  | 220M/308M [00:01<00:00, 136MB/s]Downloading pytorch_model.bin:  78%|███████▊  | 241M/308M [00:02<00:00, 140MB/s]Downloading pytorch_model.bin:  85%|████████▌ | 262M/308M [00:02<00:00, 140MB/s]Downloading pytorch_model.bin:  92%|█████████▏| 283M/308M [00:02<00:00, 141MB/s]Downloading pytorch_model.bin:  99%|█████████▉| 304M/308M [00:02<00:00, 143MB/s]Downloading pytorch_model.bin: 100%|██████████| 308M/308M [00:02<00:00, 119MB/s]
Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]Downloading (…)neration_config.json: 100%|██████████| 147/147 [00:00<00:00, 1.12MB/s]
Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 23967.45it/s]
Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 42366.71it/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 0 examples [00:00, ? examples/s]
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/t5_cte.py", line 13, in <module>
    run_experiment.run_experiment(model, tokenizer, batch_size, input_file, output_folder, experiment_count)
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 36, in run_experiment
    for experiment in range(experiment_count):
                      ^^^^^^^^^^^^^^^^^^^^^^^
TypeError: 'str' object cannot be interpreted as an integer
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/t5_cte.py", line 13, in <module>
    run_experiment.run_experiment(model, tokenizer, batch_size, input_file, output_folder, experiment_count)
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 36, in run_experiment
    for experiment in range(experiment_count):
                      ^^^^^^^^^^^^^^^^^^^^^^^
TypeError: 'str' object cannot be interpreted as an integer
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/t5_cte.py", line 13, in <module>
    run_experiment.run_experiment(model, tokenizer, batch_size, input_file, output_folder, experiment_count)
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 36, in run_experiment
    for experiment in range(experiment_count):
                      ^^^^^^^^^^^^^^^^^^^^^^^
TypeError: 'str' object cannot be interpreted as an integer
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/t5_cte.py", line 13, in <module>
    run_experiment.run_experiment(model, tokenizer, batch_size, input_file, output_folder, experiment_count)
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 36, in run_experiment
    for experiment in range(experiment_count):
                      ^^^^^^^^^^^^^^^^^^^^^^^
TypeError: 'str' object cannot be interpreted as an integer
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/t5_cte.py", line 13, in <module>
    run_experiment.run_experiments(model, tokenizer, batch_size, input_file, output_folder, experiment_count)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 36, in run_experiment
    for experiment in range(experiment_count):
                      ^^^^^^^^^^^^^^^^^^^^^^^
TypeError: 'str' object cannot be interpreted as an integer
Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]Downloading spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 172MB/s]
Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]Downloading (…)cial_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 12.2MB/s]
Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]Downloading (…)okenizer_config.json: 100%|██████████| 2.54k/2.54k [00:00<00:00, 14.0MB/s]
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Downloading (…)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|██████████| 1.40k/1.40k [00:00<00:00, 9.06MB/s]
Downloading model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]Downloading model.safetensors:   1%|          | 10.5M/990M [00:00<00:17, 56.8MB/s]Downloading model.safetensors:   2%|▏         | 21.0M/990M [00:00<00:14, 65.6MB/s]Downloading model.safetensors:   3%|▎         | 31.5M/990M [00:00<00:13, 70.0MB/s]Downloading model.safetensors:   4%|▍         | 41.9M/990M [00:00<00:11, 79.1MB/s]Downloading model.safetensors:   6%|▋         | 62.9M/990M [00:00<00:09, 96.6MB/s]Downloading model.safetensors:   8%|▊         | 83.9M/990M [00:00<00:08, 111MB/s] Downloading model.safetensors:  11%|█         | 105M/990M [00:01<00:07, 120MB/s] Downloading model.safetensors:  13%|█▎        | 126M/990M [00:01<00:09, 94.2MB/s]Downloading model.safetensors:  15%|█▍        | 147M/990M [00:01<00:07, 106MB/s] Downloading model.safetensors:  17%|█▋        | 168M/990M [00:01<00:07, 115MB/s]Downloading model.safetensors:  19%|█▉        | 189M/990M [00:01<00:06, 124MB/s]Downloading model.safetensors:  21%|██        | 210M/990M [00:01<00:06, 130MB/s]Downloading model.safetensors:  23%|██▎       | 231M/990M [00:02<00:05, 134MB/s]Downloading model.safetensors:  25%|██▌       | 252M/990M [00:02<00:05, 138MB/s]Downloading model.safetensors:  28%|██▊       | 273M/990M [00:02<00:05, 140MB/s]Downloading model.safetensors:  30%|██▉       | 294M/990M [00:03<00:10, 66.4MB/s]Downloading model.safetensors:  32%|███▏      | 315M/990M [00:03<00:08, 79.6MB/s]Downloading model.safetensors:  34%|███▍      | 336M/990M [00:03<00:07, 90.2MB/s]Downloading model.safetensors:  36%|███▌      | 357M/990M [00:03<00:06, 102MB/s] Downloading model.safetensors:  38%|███▊      | 377M/990M [00:03<00:05, 112MB/s]Downloading model.safetensors:  40%|████      | 398M/990M [00:03<00:05, 118MB/s]Downloading model.safetensors:  42%|████▏     | 419M/990M [00:03<00:04, 125MB/s]Downloading model.safetensors:  44%|████▍     | 440M/990M [00:04<00:04, 113MB/s]Downloading model.safetensors:  47%|████▋     | 461M/990M [00:04<00:06, 84.9MB/s]Downloading model.safetensors:  48%|████▊     | 472M/990M [00:04<00:07, 68.4MB/s]Downloading model.safetensors:  49%|████▊     | 482M/990M [00:05<00:07, 68.9MB/s]Downloading model.safetensors:  51%|█████     | 503M/990M [00:05<00:05, 84.0MB/s]Downloading model.safetensors:  53%|█████▎    | 524M/990M [00:05<00:04, 95.8MB/s]Downloading model.safetensors:  55%|█████▌    | 545M/990M [00:05<00:04, 107MB/s] Downloading model.safetensors:  57%|█████▋    | 566M/990M [00:05<00:03, 117MB/s]Downloading model.safetensors:  59%|█████▉    | 587M/990M [00:05<00:03, 123MB/s]Downloading model.safetensors:  61%|██████▏   | 608M/990M [00:05<00:02, 130MB/s]Downloading model.safetensors:  64%|██████▎   | 629M/990M [00:06<00:03, 92.1MB/s]Downloading model.safetensors:  66%|██████▌   | 650M/990M [00:06<00:04, 74.3MB/s]Downloading model.safetensors:  68%|██████▊   | 671M/990M [00:06<00:03, 86.6MB/s]Downloading model.safetensors:  70%|██████▉   | 692M/990M [00:07<00:03, 98.5MB/s]Downloading model.safetensors:  72%|███████▏  | 713M/990M [00:07<00:02, 109MB/s] Downloading model.safetensors:  74%|███████▍  | 734M/990M [00:07<00:02, 115MB/s]Downloading model.safetensors:  76%|███████▌  | 755M/990M [00:07<00:01, 123MB/s]Downloading model.safetensors:  78%|███████▊  | 776M/990M [00:07<00:01, 129MB/s]Downloading model.safetensors:  80%|████████  | 797M/990M [00:07<00:01, 134MB/s]Downloading model.safetensors:  83%|████████▎ | 818M/990M [00:08<00:01, 108MB/s]Downloading model.safetensors:  85%|████████▍ | 839M/990M [00:08<00:01, 116MB/s]Downloading model.safetensors:  87%|████████▋ | 860M/990M [00:08<00:01, 123MB/s]Downloading model.safetensors:  89%|████████▉ | 881M/990M [00:08<00:00, 128MB/s]Downloading model.safetensors:  91%|█████████ | 902M/990M [00:08<00:00, 133MB/s]Downloading model.safetensors:  93%|█████████▎| 923M/990M [00:08<00:00, 138MB/s]Downloading model.safetensors:  95%|█████████▌| 944M/990M [00:08<00:00, 138MB/s]Downloading model.safetensors:  97%|█████████▋| 965M/990M [00:09<00:00, 140MB/s]Downloading model.safetensors: 100%|█████████▉| 986M/990M [00:09<00:00, 95.0MB/s]Downloading model.safetensors: 100%|██████████| 990M/990M [00:09<00:00, 105MB/s] 
Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]Downloading (…)neration_config.json: 100%|██████████| 147/147 [00:00<00:00, 768kB/s]
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/t5_cte.py", line 13, in <module>
    run_experiment.run_experiments(model, tokenizer, batch_size, input_file, output_folder, experiment_count)
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 36, in run_experiments
    for experiment in range(int(experiment_count)):
                      ^^^^^^^^^^^^^^^^^^^^^^^
TypeError: 'str' object cannot be interpreted as an integer
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/t5_cte.py", line 13, in <module>
    run_experiment.run_experiments(model, tokenizer, batch_size, input_file, output_folder, experiment_count)
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 37, in run_experiments
    predictions = _evaluate_model(model, tokenizer, dataset, batch_size)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 24, in _evaluate_model
    encodings = _generate_encodings(tokenizer, dataset_shard)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 10, in _generate_encodings
    encodings = tokenizer(dataset["question"], padding=True, truncation=True, return_tensors="pt").to(device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2602, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2688, in _call_one
    return self.batch_encode_plus(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2879, in batch_encode_plus
    return self._batch_encode_plus(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils.py", line 744, in _batch_encode_plus
    batch_outputs = self._batch_prepare_for_model(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils.py", line 816, in _batch_prepare_for_model
    batch_outputs = self.pad(
                    ^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 3018, in pad
    raise ValueError(
ValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided []
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/t5_cte.py", line 13, in <module>
    run_experiment.run_experiments(model, tokenizer, batch_size, input_file, output_folder, experiment_count)
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 37, in run_experiments
    predictions = _evaluate_model(model, tokenizer, dataset, batch_size)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 24, in _evaluate_model
    encodings = _generate_encodings(tokenizer, dataset_shard)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 10, in _generate_encodings
    encodings = tokenizer(dataset["question"], padding=True, truncation=True, return_tensors="pt").to(device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2602, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2688, in _call_one
    return self.batch_encode_plus(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2879, in batch_encode_plus
    return self._batch_encode_plus(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils.py", line 744, in _batch_encode_plus
    batch_outputs = self._batch_prepare_for_model(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils.py", line 816, in _batch_prepare_for_model
    batch_outputs = self.pad(
                    ^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 3018, in pad
    raise ValueError(
ValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided []
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/t5_cte.py", line 13, in <module>
    run_experiment.run_experiments(model, tokenizer, batch_size, input_file, output_folder, experiment_count)
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 37, in run_experiments
    predictions = _evaluate_model(model, tokenizer, dataset, batch_size)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 24, in _evaluate_model
    encodings = _generate_encodings(tokenizer, dataset_shard)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 10, in _generate_encodings
    encodings = tokenizer(dataset["question"], padding=True, truncation=True, return_tensors="pt").to(device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2602, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2688, in _call_one
    return self.batch_encode_plus(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2879, in batch_encode_plus
    return self._batch_encode_plus(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils.py", line 744, in _batch_encode_plus
    batch_outputs = self._batch_prepare_for_model(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils.py", line 816, in _batch_prepare_for_model
    batch_outputs = self.pad(
                    ^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 3018, in pad
    raise ValueError(
ValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided []
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/t5_cte.py", line 13, in <module>
    run_experiment.run_experiments(model, tokenizer, batch_size, input_file, output_folder, experiment_count)
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 37, in run_experiments
    predictions = _evaluate_model(model, tokenizer, dataset, batch_size)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 24, in _evaluate_model
    encodings = _generate_encodings(tokenizer, dataset_shard)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 10, in _generate_encodings
    encodings = tokenizer(dataset["question"], padding=True, truncation=True, return_tensors="pt").to(device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2602, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2688, in _call_one
    return self.batch_encode_plus(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2879, in batch_encode_plus
    return self._batch_encode_plus(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils.py", line 744, in _batch_encode_plus
    batch_outputs = self._batch_prepare_for_model(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils.py", line 816, in _batch_prepare_for_model
    batch_outputs = self.pad(
                    ^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 3018, in pad
    raise ValueError(
ValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided []
Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]Downloading spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 141MB/s]
Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]Downloading (…)cial_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 13.3MB/s]
Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]Downloading (…)okenizer_config.json: 100%|██████████| 2.54k/2.54k [00:00<00:00, 16.6MB/s]
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Downloading (…)lve/main/config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|██████████| 662/662 [00:00<00:00, 7.23MB/s]
Downloading model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]Downloading model.safetensors:   0%|          | 10.5M/3.13G [00:00<01:27, 35.9MB/s]Downloading model.safetensors:   1%|          | 21.0M/3.13G [00:00<01:06, 46.8MB/s]Downloading model.safetensors:   1%|          | 31.5M/3.13G [00:00<00:55, 56.3MB/s]Downloading model.safetensors:   1%|▏         | 41.9M/3.13G [00:00<00:46, 66.5MB/s]Downloading model.safetensors:   2%|▏         | 52.4M/3.13G [00:00<00:41, 74.3MB/s]Downloading model.safetensors:   2%|▏         | 73.4M/3.13G [00:00<00:32, 94.3MB/s]Downloading model.safetensors:   3%|▎         | 83.9M/3.13G [00:01<00:38, 78.8MB/s]Downloading model.safetensors:   3%|▎         | 105M/3.13G [00:01<00:30, 97.7MB/s] Downloading model.safetensors:   4%|▍         | 126M/3.13G [00:01<00:26, 112MB/s] Downloading model.safetensors:   5%|▍         | 147M/3.13G [00:01<00:25, 119MB/s]Downloading model.safetensors:   5%|▌         | 168M/3.13G [00:01<00:23, 127MB/s]Downloading model.safetensors:   6%|▌         | 189M/3.13G [00:01<00:22, 132MB/s]Downloading model.safetensors:   7%|▋         | 210M/3.13G [00:02<00:21, 136MB/s]Downloading model.safetensors:   7%|▋         | 231M/3.13G [00:02<00:27, 105MB/s]Downloading model.safetensors:   8%|▊         | 252M/3.13G [00:02<00:43, 66.0MB/s]Downloading model.safetensors:   9%|▊         | 273M/3.13G [00:03<00:37, 77.2MB/s]Downloading model.safetensors:   9%|▉         | 283M/3.13G [00:03<00:35, 80.0MB/s]Downloading model.safetensors:  10%|▉         | 304M/3.13G [00:03<00:30, 93.7MB/s]Downloading model.safetensors:  10%|█         | 325M/3.13G [00:03<00:26, 106MB/s] Downloading model.safetensors:  11%|█         | 346M/3.13G [00:03<00:24, 116MB/s]Downloading model.safetensors:  12%|█▏        | 367M/3.13G [00:03<00:22, 123MB/s]Downloading model.safetensors:  12%|█▏        | 388M/3.13G [00:03<00:22, 125MB/s]Downloading model.safetensors:  13%|█▎        | 409M/3.13G [00:04<00:20, 130MB/s]Downloading model.safetensors:  14%|█▎        | 430M/3.13G [00:04<00:27, 97.3MB/s]Downloading model.safetensors:  14%|█▍        | 451M/3.13G [00:05<00:41, 64.2MB/s]Downloading model.safetensors:  15%|█▌        | 472M/3.13G [00:05<00:35, 75.9MB/s]Downloading model.safetensors:  16%|█▌        | 493M/3.13G [00:05<00:29, 88.6MB/s]Downloading model.safetensors:  16%|█▋        | 514M/3.13G [00:05<00:26, 100MB/s] Downloading model.safetensors:  17%|█▋        | 535M/3.13G [00:05<00:23, 111MB/s]Downloading model.safetensors:  18%|█▊        | 556M/3.13G [00:05<00:21, 120MB/s]Downloading model.safetensors:  18%|█▊        | 577M/3.13G [00:05<00:20, 126MB/s]Downloading model.safetensors:  19%|█▉        | 598M/3.13G [00:06<00:23, 108MB/s]Downloading model.safetensors:  20%|█▉        | 619M/3.13G [00:06<00:25, 96.9MB/s]Downloading model.safetensors:  20%|██        | 640M/3.13G [00:06<00:34, 73.3MB/s]Downloading model.safetensors:  21%|██        | 650M/3.13G [00:07<00:35, 69.0MB/s]Downloading model.safetensors:  21%|██        | 661M/3.13G [00:07<00:34, 72.5MB/s]Downloading model.safetensors:  22%|██▏       | 682M/3.13G [00:07<00:27, 88.4MB/s]Downloading model.safetensors:  22%|██▏       | 703M/3.13G [00:07<00:23, 102MB/s] Downloading model.safetensors:  23%|██▎       | 724M/3.13G [00:07<00:21, 112MB/s]Downloading model.safetensors:  24%|██▍       | 744M/3.13G [00:07<00:19, 121MB/s]Downloading model.safetensors:  24%|██▍       | 765M/3.13G [00:07<00:18, 126MB/s]Downloading model.safetensors:  25%|██▌       | 786M/3.13G [00:08<00:17, 133MB/s]Downloading model.safetensors:  26%|██▌       | 807M/3.13G [00:08<00:31, 74.8MB/s]Downloading model.safetensors:  26%|██▋       | 828M/3.13G [00:09<00:35, 64.1MB/s]Downloading model.safetensors:  27%|██▋       | 839M/3.13G [00:09<00:35, 64.1MB/s]Downloading model.safetensors:  27%|██▋       | 860M/3.13G [00:09<00:29, 76.1MB/s]Downloading model.safetensors:  28%|██▊       | 881M/3.13G [00:09<00:25, 88.6MB/s]Downloading model.safetensors:  29%|██▉       | 902M/3.13G [00:09<00:22, 101MB/s] Downloading model.safetensors:  29%|██▉       | 923M/3.13G [00:09<00:20, 110MB/s]Downloading model.safetensors:  30%|███       | 944M/3.13G [00:10<00:18, 117MB/s]Downloading model.safetensors:  31%|███       | 965M/3.13G [00:10<00:17, 125MB/s]Downloading model.safetensors:  31%|███▏      | 986M/3.13G [00:10<00:19, 109MB/s]Downloading model.safetensors:  32%|███▏      | 1.01G/3.13G [00:10<00:28, 73.7MB/s]Downloading model.safetensors:  32%|███▏      | 1.02G/3.13G [00:11<00:35, 59.0MB/s]Downloading model.safetensors:  33%|███▎      | 1.04G/3.13G [00:11<00:28, 74.2MB/s]Downloading model.safetensors:  34%|███▍      | 1.06G/3.13G [00:11<00:23, 87.6MB/s]Downloading model.safetensors:  34%|███▍      | 1.08G/3.13G [00:11<00:20, 99.3MB/s]Downloading model.safetensors:  35%|███▌      | 1.10G/3.13G [00:11<00:18, 109MB/s] Downloading model.safetensors:  36%|███▌      | 1.12G/3.13G [00:12<00:17, 112MB/s]Downloading model.safetensors:  36%|███▋      | 1.14G/3.13G [00:12<00:16, 117MB/s]Downloading model.safetensors:  37%|███▋      | 1.16G/3.13G [00:12<00:22, 87.3MB/s]Downloading model.safetensors:  37%|███▋      | 1.17G/3.13G [00:12<00:26, 75.1MB/s]Downloading model.safetensors:  38%|███▊      | 1.18G/3.13G [00:13<00:33, 57.6MB/s]Downloading model.safetensors:  38%|███▊      | 1.21G/3.13G [00:13<00:26, 73.8MB/s]Downloading model.safetensors:  39%|███▉      | 1.23G/3.13G [00:13<00:21, 88.3MB/s]Downloading model.safetensors:  40%|███▉      | 1.25G/3.13G [00:13<00:18, 101MB/s] Downloading model.safetensors:  41%|████      | 1.27G/3.13G [00:13<00:16, 110MB/s]Downloading model.safetensors:  41%|████      | 1.29G/3.13G [00:13<00:15, 119MB/s]Downloading model.safetensors:  42%|████▏     | 1.31G/3.13G [00:14<00:14, 126MB/s]Downloading model.safetensors:  43%|████▎     | 1.33G/3.13G [00:14<00:13, 131MB/s]Downloading model.safetensors:  43%|████▎     | 1.35G/3.13G [00:14<00:23, 76.9MB/s]Downloading model.safetensors:  44%|████▍     | 1.37G/3.13G [00:15<00:23, 73.5MB/s]Downloading model.safetensors:  45%|████▍     | 1.39G/3.13G [00:15<00:20, 86.3MB/s]Downloading model.safetensors:  45%|████▌     | 1.42G/3.13G [00:15<00:19, 90.4MB/s]Downloading model.safetensors:  46%|████▌     | 1.44G/3.13G [00:15<00:20, 81.1MB/s]Downloading model.safetensors:  47%|████▋     | 1.46G/3.13G [00:15<00:18, 90.8MB/s]Downloading model.safetensors:  47%|████▋     | 1.48G/3.13G [00:16<00:16, 102MB/s] Downloading model.safetensors:  48%|████▊     | 1.50G/3.13G [00:16<00:14, 112MB/s]Downloading model.safetensors:  49%|████▊     | 1.52G/3.13G [00:16<00:13, 119MB/s]Downloading model.safetensors:  49%|████▉     | 1.54G/3.13G [00:16<00:12, 126MB/s]Downloading model.safetensors:  50%|████▉     | 1.56G/3.13G [00:16<00:12, 130MB/s]Downloading model.safetensors:  51%|█████     | 1.58G/3.13G [00:17<00:18, 82.9MB/s]Downloading model.safetensors:  51%|█████     | 1.60G/3.13G [00:17<00:16, 91.0MB/s]Downloading model.safetensors:  52%|█████▏    | 1.63G/3.13G [00:17<00:14, 101MB/s] Downloading model.safetensors:  53%|█████▎    | 1.65G/3.13G [00:17<00:13, 108MB/s]Downloading model.safetensors:  53%|█████▎    | 1.67G/3.13G [00:17<00:12, 115MB/s]Downloading model.safetensors:  54%|█████▍    | 1.69G/3.13G [00:17<00:11, 123MB/s]Downloading model.safetensors:  55%|█████▍    | 1.71G/3.13G [00:17<00:11, 128MB/s]Downloading model.safetensors:  55%|█████▌    | 1.73G/3.13G [00:18<00:14, 95.1MB/s]Downloading model.safetensors:  56%|█████▌    | 1.75G/3.13G [00:18<00:20, 68.1MB/s]Downloading model.safetensors:  56%|█████▌    | 1.76G/3.13G [00:19<00:19, 69.1MB/s]Downloading model.safetensors:  57%|█████▋    | 1.78G/3.13G [00:19<00:16, 81.9MB/s]Downloading model.safetensors:  58%|█████▊    | 1.80G/3.13G [00:19<00:14, 94.9MB/s]Downloading model.safetensors:  58%|█████▊    | 1.82G/3.13G [00:19<00:12, 107MB/s] Downloading model.safetensors:  59%|█████▉    | 1.85G/3.13G [00:19<00:11, 115MB/s]Downloading model.safetensors:  60%|█████▉    | 1.87G/3.13G [00:19<00:10, 122MB/s]Downloading model.safetensors:  60%|██████    | 1.89G/3.13G [00:19<00:09, 129MB/s]Downloading model.safetensors:  61%|██████    | 1.91G/3.13G [00:20<00:13, 88.4MB/s]Downloading model.safetensors:  62%|██████▏   | 1.93G/3.13G [00:20<00:16, 71.6MB/s]Downloading model.safetensors:  62%|██████▏   | 1.94G/3.13G [00:20<00:17, 69.2MB/s]Downloading model.safetensors:  63%|██████▎   | 1.96G/3.13G [00:21<00:14, 81.2MB/s]Downloading model.safetensors:  63%|██████▎   | 1.98G/3.13G [00:21<00:12, 93.8MB/s]Downloading model.safetensors:  64%|██████▍   | 2.00G/3.13G [00:21<00:13, 86.6MB/s]Downloading model.safetensors:  65%|██████▍   | 2.02G/3.13G [00:21<00:10, 105MB/s] Downloading model.safetensors:  65%|██████▌   | 2.04G/3.13G [00:21<00:09, 114MB/s]Downloading model.safetensors:  66%|██████▌   | 2.07G/3.13G [00:21<00:08, 122MB/s]Downloading model.safetensors:  67%|██████▋   | 2.09G/3.13G [00:22<00:16, 65.2MB/s]Downloading model.safetensors:  67%|██████▋   | 2.10G/3.13G [00:22<00:16, 63.4MB/s]Downloading model.safetensors:  68%|██████▊   | 2.12G/3.13G [00:22<00:13, 75.7MB/s]Downloading model.safetensors:  68%|██████▊   | 2.14G/3.13G [00:23<00:11, 88.8MB/s]Downloading model.safetensors:  69%|██████▉   | 2.16G/3.13G [00:23<00:09, 101MB/s] Downloading model.safetensors:  70%|██████▉   | 2.18G/3.13G [00:23<00:08, 110MB/s]Downloading model.safetensors:  70%|███████   | 2.20G/3.13G [00:23<00:07, 119MB/s]Downloading model.safetensors:  71%|███████   | 2.22G/3.13G [00:23<00:07, 126MB/s]Downloading model.safetensors:  72%|███████▏  | 2.24G/3.13G [00:24<00:11, 80.0MB/s]Downloading model.safetensors:  72%|███████▏  | 2.26G/3.13G [00:24<00:10, 83.6MB/s]Downloading model.safetensors:  73%|███████▎  | 2.28G/3.13G [00:24<00:12, 68.0MB/s]Downloading model.safetensors:  73%|███████▎  | 2.30G/3.13G [00:24<00:10, 82.4MB/s]Downloading model.safetensors:  74%|███████▍  | 2.32G/3.13G [00:24<00:08, 95.4MB/s]Downloading model.safetensors:  75%|███████▍  | 2.34G/3.13G [00:25<00:07, 105MB/s] Downloading model.safetensors:  75%|███████▌  | 2.36G/3.13G [00:25<00:06, 113MB/s]Downloading model.safetensors:  76%|███████▌  | 2.38G/3.13G [00:25<00:06, 120MB/s]Downloading model.safetensors:  77%|███████▋  | 2.40G/3.13G [00:25<00:05, 127MB/s]Downloading model.safetensors:  77%|███████▋  | 2.42G/3.13G [00:25<00:07, 98.9MB/s]Downloading model.safetensors:  78%|███████▊  | 2.44G/3.13G [00:26<00:09, 72.8MB/s]Downloading model.safetensors:  78%|███████▊  | 2.45G/3.13G [00:26<00:11, 59.8MB/s]Downloading model.safetensors:  79%|███████▉  | 2.47G/3.13G [00:26<00:08, 73.5MB/s]Downloading model.safetensors:  80%|███████▉  | 2.50G/3.13G [00:26<00:07, 86.3MB/s]Downloading model.safetensors:  80%|████████  | 2.52G/3.13G [00:27<00:06, 98.8MB/s]Downloading model.safetensors:  81%|████████  | 2.54G/3.13G [00:27<00:05, 109MB/s] Downloading model.safetensors:  82%|████████▏ | 2.56G/3.13G [00:27<00:04, 118MB/s]Downloading model.safetensors:  82%|████████▏ | 2.58G/3.13G [00:27<00:04, 125MB/s]Downloading model.safetensors:  83%|████████▎ | 2.60G/3.13G [00:27<00:04, 130MB/s]Downloading model.safetensors:  84%|████████▎ | 2.62G/3.13G [00:28<00:05, 97.5MB/s]Downloading model.safetensors:  84%|████████▍ | 2.64G/3.13G [00:28<00:08, 57.5MB/s]Downloading model.safetensors:  85%|████████▌ | 2.66G/3.13G [00:28<00:06, 70.3MB/s]Downloading model.safetensors:  86%|████████▌ | 2.68G/3.13G [00:29<00:05, 82.4MB/s]Downloading model.safetensors:  86%|████████▋ | 2.71G/3.13G [00:29<00:04, 94.3MB/s]Downloading model.safetensors:  87%|████████▋ | 2.73G/3.13G [00:29<00:03, 102MB/s] Downloading model.safetensors:  88%|████████▊ | 2.75G/3.13G [00:29<00:03, 111MB/s]Downloading model.safetensors:  88%|████████▊ | 2.77G/3.13G [00:29<00:03, 118MB/s]Downloading model.safetensors:  89%|████████▉ | 2.79G/3.13G [00:30<00:03, 91.0MB/s]Downloading model.safetensors:  90%|████████▉ | 2.81G/3.13G [00:30<00:04, 76.8MB/s]Downloading model.safetensors:  90%|█████████ | 2.83G/3.13G [00:30<00:04, 73.4MB/s]Downloading model.safetensors:  91%|█████████ | 2.84G/3.13G [00:30<00:04, 70.5MB/s]Downloading model.safetensors:  91%|█████████ | 2.85G/3.13G [00:30<00:03, 74.1MB/s]Downloading model.safetensors:  92%|█████████▏| 2.87G/3.13G [00:31<00:02, 88.6MB/s]Downloading model.safetensors:  92%|█████████▏| 2.89G/3.13G [00:31<00:02, 102MB/s] Downloading model.safetensors:  93%|█████████▎| 2.92G/3.13G [00:31<00:01, 113MB/s]Downloading model.safetensors:  94%|█████████▎| 2.94G/3.13G [00:31<00:01, 122MB/s]Downloading model.safetensors:  94%|█████████▍| 2.96G/3.13G [00:31<00:01, 128MB/s]Downloading model.safetensors:  95%|█████████▌| 2.98G/3.13G [00:31<00:01, 133MB/s]Downloading model.safetensors:  96%|█████████▌| 3.00G/3.13G [00:32<00:01, 115MB/s]Downloading model.safetensors:  96%|█████████▋| 3.02G/3.13G [00:32<00:01, 60.7MB/s]Downloading model.safetensors:  97%|█████████▋| 3.04G/3.13G [00:32<00:01, 72.7MB/s]Downloading model.safetensors:  98%|█████████▊| 3.06G/3.13G [00:33<00:00, 84.3MB/s]Downloading model.safetensors:  98%|█████████▊| 3.08G/3.13G [00:33<00:00, 93.7MB/s]Downloading model.safetensors:  99%|█████████▉| 3.10G/3.13G [00:33<00:00, 105MB/s] Downloading model.safetensors: 100%|█████████▉| 3.12G/3.13G [00:33<00:00, 115MB/s]Downloading model.safetensors: 100%|██████████| 3.13G/3.13G [00:33<00:00, 93.1MB/s]
Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]Downloading (…)neration_config.json: 100%|██████████| 147/147 [00:00<00:00, 918kB/s]
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/t5_cte.py", line 13, in <module>
    run_experiment.run_experiments(model, tokenizer, batch_size, input_file, output_folder, experiment_count)
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 37, in run_experiments
    predictions = _evaluate_model(model, tokenizer, dataset, batch_size)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 24, in _evaluate_model
    encodings = _generate_encodings(tokenizer, dataset_shard)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 10, in _generate_encodings
    encodings = tokenizer(dataset["question"], padding=True, truncation=True, return_tensors="pt").to(device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2602, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2688, in _call_one
    return self.batch_encode_plus(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2879, in batch_encode_plus
    return self._batch_encode_plus(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils.py", line 744, in _batch_encode_plus
    batch_outputs = self._batch_prepare_for_model(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils.py", line 816, in _batch_prepare_for_model
    batch_outputs = self.pad(
                    ^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 3018, in pad
    raise ValueError(
ValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided []
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/t5_cte.py", line 13, in <module>
    run_experiment.run_experiments(model, tokenizer, batch_size, input_file, output_folder, experiment_count)
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 37, in run_experiments
    predictions = _evaluate_model(model, tokenizer, dataset, batch_size)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 24, in _evaluate_model
    encodings = _generate_encodings(tokenizer, dataset_shard)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 10, in _generate_encodings
    encodings = tokenizer(dataset["question"], padding=True, truncation=True, return_tensors="pt").to(device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2602, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2688, in _call_one
    return self.batch_encode_plus(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2879, in batch_encode_plus
    return self._batch_encode_plus(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils.py", line 744, in _batch_encode_plus
    batch_outputs = self._batch_prepare_for_model(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils.py", line 816, in _batch_prepare_for_model
    batch_outputs = self.pad(
                    ^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 3018, in pad
    raise ValueError(
ValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided []
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/t5_cte.py", line 13, in <module>
    run_experiment.run_experiments(model, tokenizer, batch_size, input_file, output_folder, experiment_count)
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 37, in run_experiments
    predictions = _evaluate_model(model, tokenizer, dataset, batch_size)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 24, in _evaluate_model
    encodings = _generate_encodings(tokenizer, dataset_shard)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 10, in _generate_encodings
    encodings = tokenizer(dataset["question"], padding=True, truncation=True, return_tensors="pt").to(device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2602, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2688, in _call_one
    return self.batch_encode_plus(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2879, in batch_encode_plus
    return self._batch_encode_plus(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils.py", line 744, in _batch_encode_plus
    batch_outputs = self._batch_prepare_for_model(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils.py", line 816, in _batch_prepare_for_model
    batch_outputs = self.pad(
                    ^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 3018, in pad
    raise ValueError(
ValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided []
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/t5_cte.py", line 13, in <module>
    run_experiment.run_experiments(model, tokenizer, batch_size, input_file, output_folder, experiment_count)
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 37, in run_experiments
    predictions = _evaluate_model(model, tokenizer, dataset, batch_size)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 24, in _evaluate_model
    encodings = _generate_encodings(tokenizer, dataset_shard)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 10, in _generate_encodings
    encodings = tokenizer(dataset["question"], padding=True, truncation=True, return_tensors="pt").to(device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2602, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2688, in _call_one
    return self.batch_encode_plus(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2879, in batch_encode_plus
    return self._batch_encode_plus(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils.py", line 744, in _batch_encode_plus
    batch_outputs = self._batch_prepare_for_model(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils.py", line 816, in _batch_prepare_for_model
    batch_outputs = self.pad(
                    ^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 3018, in pad
    raise ValueError(
ValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided []
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/t5_cte.py", line 13, in <module>
    run_experiment.run_experiments(model, tokenizer, batch_size, input_file, output_folder, experiment_count)
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 37, in run_experiments
    predictions = _evaluate_model(model, tokenizer, dataset, batch_size)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 24, in _evaluate_model
    encodings = _generate_encodings(tokenizer, dataset_shard)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiment.py", line 10, in _generate_encodings
    encodings = tokenizer(dataset["question"], padding=True, truncation=True, return_tensors="pt").to(device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2602, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2688, in _call_one
    return self.batch_encode_plus(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2879, in batch_encode_plus
    return self._batch_encode_plus(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils.py", line 744, in _batch_encode_plus
    batch_outputs = self._batch_prepare_for_model(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils.py", line 816, in _batch_prepare_for_model
    batch_outputs = self.pad(
                    ^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 3018, in pad
    raise ValueError(
ValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided []
Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]Downloading spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 76.9MB/s]
Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]Downloading (…)cial_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 11.2MB/s]
Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]Downloading (…)okenizer_config.json: 100%|██████████| 2.54k/2.54k [00:00<00:00, 13.9MB/s]
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Downloading (…)lve/main/config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|██████████| 1.44k/1.44k [00:00<00:00, 13.7MB/s]
Downloading (…)model.bin.index.json:   0%|          | 0.00/50.8k [00:00<?, ?B/s]Downloading (…)model.bin.index.json: 100%|██████████| 50.8k/50.8k [00:00<00:00, 159MB/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.45G [00:00<?, ?B/s][A
Downloading (…)l-00001-of-00002.bin:   0%|          | 10.5M/9.45G [00:00<02:02, 76.9MB/s][A
Downloading (…)l-00001-of-00002.bin:   0%|          | 21.0M/9.45G [00:00<02:21, 66.7MB/s][A
Downloading (…)l-00001-of-00002.bin:   0%|          | 31.5M/9.45G [00:00<02:08, 73.2MB/s][A
Downloading (…)l-00001-of-00002.bin:   0%|          | 41.9M/9.45G [00:00<02:10, 72.1MB/s][A
Downloading (…)l-00001-of-00002.bin:   1%|          | 52.4M/9.45G [00:00<02:05, 75.1MB/s][A
Downloading (…)l-00001-of-00002.bin:   1%|          | 62.9M/9.45G [00:00<02:13, 70.5MB/s][A
Downloading (…)l-00001-of-00002.bin:   1%|          | 73.4M/9.45G [00:01<02:07, 73.6MB/s][A
Downloading (…)l-00001-of-00002.bin:   1%|          | 83.9M/9.45G [00:01<02:03, 75.8MB/s][A
Downloading (…)l-00001-of-00002.bin:   1%|          | 94.4M/9.45G [00:01<02:11, 71.1MB/s][A
Downloading (…)l-00001-of-00002.bin:   1%|          | 105M/9.45G [00:01<02:10, 71.4MB/s] [A
Downloading (…)l-00001-of-00002.bin:   1%|          | 115M/9.45G [00:01<02:24, 64.7MB/s][A
Downloading (…)l-00001-of-00002.bin:   1%|▏         | 126M/9.45G [00:01<02:31, 61.4MB/s][A
Downloading (…)l-00001-of-00002.bin:   1%|▏         | 136M/9.45G [00:01<02:22, 65.5MB/s][A
Downloading (…)l-00001-of-00002.bin:   2%|▏         | 147M/9.45G [00:02<02:16, 68.1MB/s][A
Downloading (…)l-00001-of-00002.bin:   2%|▏         | 157M/9.45G [00:02<02:27, 62.9MB/s][A
Downloading (…)l-00001-of-00002.bin:   2%|▏         | 168M/9.45G [00:02<02:54, 53.3MB/s][A
Downloading (…)l-00001-of-00002.bin:   2%|▏         | 178M/9.45G [00:02<02:45, 56.0MB/s][A
Downloading (…)l-00001-of-00002.bin:   2%|▏         | 189M/9.45G [00:02<03:02, 50.7MB/s][A
Downloading (…)l-00001-of-00002.bin:   2%|▏         | 199M/9.45G [00:03<02:40, 57.7MB/s][A
Downloading (…)l-00001-of-00002.bin:   2%|▏         | 210M/9.45G [00:03<02:37, 58.8MB/s][A
Downloading (…)l-00001-of-00002.bin:   2%|▏         | 220M/9.45G [00:03<02:28, 62.1MB/s][A
Downloading (…)l-00001-of-00002.bin:   2%|▏         | 231M/9.45G [00:03<02:40, 57.5MB/s][A
Downloading (…)l-00001-of-00002.bin:   3%|▎         | 241M/9.45G [00:03<02:28, 62.2MB/s][A
Downloading (…)l-00001-of-00002.bin:   3%|▎         | 252M/9.45G [00:03<02:22, 64.5MB/s][A
Downloading (…)l-00001-of-00002.bin:   3%|▎         | 262M/9.45G [00:04<02:42, 56.5MB/s][A
Downloading (…)l-00001-of-00002.bin:   3%|▎         | 273M/9.45G [00:04<02:29, 61.2MB/s][A
Downloading (…)l-00001-of-00002.bin:   3%|▎         | 283M/9.45G [00:04<02:35, 58.8MB/s][A
Downloading (…)l-00001-of-00002.bin:   3%|▎         | 294M/9.45G [00:04<02:22, 64.5MB/s][A
Downloading (…)l-00001-of-00002.bin:   3%|▎         | 304M/9.45G [00:04<02:13, 68.6MB/s][A
Downloading (…)l-00001-of-00002.bin:   3%|▎         | 315M/9.45G [00:04<02:07, 71.7MB/s][A
Downloading (…)l-00001-of-00002.bin:   3%|▎         | 325M/9.45G [00:05<02:01, 75.0MB/s][A
Downloading (…)l-00001-of-00002.bin:   4%|▎         | 336M/9.45G [00:05<01:57, 77.6MB/s][A
Downloading (…)l-00001-of-00002.bin:   4%|▎         | 346M/9.45G [00:05<02:06, 72.1MB/s][A
Downloading (…)l-00001-of-00002.bin:   4%|▍         | 357M/9.45G [00:05<02:04, 72.8MB/s][A
Downloading (…)l-00001-of-00002.bin:   4%|▍         | 367M/9.45G [00:05<02:11, 69.3MB/s][A
Downloading (…)l-00001-of-00002.bin:   4%|▍         | 377M/9.45G [00:05<02:04, 73.1MB/s][A
Downloading (…)l-00001-of-00002.bin:   4%|▍         | 388M/9.45G [00:05<02:00, 75.0MB/s][A
Downloading (…)l-00001-of-00002.bin:   4%|▍         | 398M/9.45G [00:05<01:55, 78.7MB/s][A
Downloading (…)l-00001-of-00002.bin:   4%|▍         | 409M/9.45G [00:06<01:53, 79.8MB/s][A
Downloading (…)l-00001-of-00002.bin:   4%|▍         | 419M/9.45G [00:06<01:50, 81.9MB/s][A
Downloading (…)l-00001-of-00002.bin:   5%|▍         | 430M/9.45G [00:06<01:50, 81.9MB/s][A
Downloading (…)l-00001-of-00002.bin:   5%|▍         | 440M/9.45G [00:06<01:50, 81.6MB/s][A
Downloading (…)l-00001-of-00002.bin:   5%|▍         | 451M/9.45G [00:06<02:11, 68.3MB/s][A
Downloading (…)l-00001-of-00002.bin:   5%|▍         | 461M/9.45G [00:06<02:07, 70.7MB/s][A
Downloading (…)l-00001-of-00002.bin:   5%|▍         | 472M/9.45G [00:06<02:00, 74.3MB/s][A
Downloading (…)l-00001-of-00002.bin:   5%|▌         | 482M/9.45G [00:07<02:01, 74.0MB/s][A
Downloading (…)l-00001-of-00002.bin:   5%|▌         | 493M/9.45G [00:07<01:55, 77.2MB/s][A
Downloading (…)l-00001-of-00002.bin:   5%|▌         | 503M/9.45G [00:07<02:07, 70.4MB/s][A
Downloading (…)l-00001-of-00002.bin:   5%|▌         | 514M/9.45G [00:07<02:37, 56.8MB/s][A
Downloading (…)l-00001-of-00002.bin:   6%|▌         | 524M/9.45G [00:07<02:23, 62.0MB/s][A
Downloading (…)l-00001-of-00002.bin:   6%|▌         | 535M/9.45G [00:07<02:14, 66.2MB/s][A
Downloading (…)l-00001-of-00002.bin:   6%|▌         | 545M/9.45G [00:08<02:14, 66.1MB/s][A
Downloading (…)l-00001-of-00002.bin:   6%|▌         | 556M/9.45G [00:08<02:39, 55.9MB/s][A
Downloading (…)l-00001-of-00002.bin:   6%|▌         | 566M/9.45G [00:08<02:32, 58.2MB/s][A
Downloading (…)l-00001-of-00002.bin:   6%|▌         | 577M/9.45G [00:08<02:19, 63.7MB/s][A
Downloading (…)l-00001-of-00002.bin:   6%|▌         | 587M/9.45G [00:08<02:08, 69.0MB/s][A
Downloading (…)l-00001-of-00002.bin:   6%|▋         | 598M/9.45G [00:08<02:00, 73.7MB/s][A
Downloading (…)l-00001-of-00002.bin:   6%|▋         | 608M/9.45G [00:09<02:00, 73.7MB/s][A
Downloading (…)l-00001-of-00002.bin:   7%|▋         | 619M/9.45G [00:09<01:56, 75.6MB/s][A
Downloading (…)l-00001-of-00002.bin:   7%|▋         | 629M/9.45G [00:09<01:52, 78.2MB/s][A
Downloading (…)l-00001-of-00002.bin:   7%|▋         | 640M/9.45G [00:09<01:52, 78.2MB/s][A
Downloading (…)l-00001-of-00002.bin:   7%|▋         | 650M/9.45G [00:09<01:49, 80.3MB/s][A
Downloading (…)l-00001-of-00002.bin:   7%|▋         | 661M/9.45G [00:09<01:58, 73.9MB/s][A
Downloading (…)l-00001-of-00002.bin:   7%|▋         | 671M/9.45G [00:09<01:53, 77.2MB/s][A
Downloading (…)l-00001-of-00002.bin:   7%|▋         | 682M/9.45G [00:09<01:56, 75.0MB/s][A
Downloading (…)l-00001-of-00002.bin:   7%|▋         | 692M/9.45G [00:10<01:52, 77.8MB/s][A
Downloading (…)l-00001-of-00002.bin:   7%|▋         | 703M/9.45G [00:10<01:53, 77.0MB/s][A
Downloading (…)l-00001-of-00002.bin:   8%|▊         | 713M/9.45G [00:10<01:51, 78.4MB/s][A
Downloading (…)l-00001-of-00002.bin:   8%|▊         | 724M/9.45G [00:10<02:02, 71.3MB/s][A
Downloading (…)l-00001-of-00002.bin:   8%|▊         | 734M/9.45G [00:10<02:02, 70.9MB/s][A
Downloading (…)l-00001-of-00002.bin:   8%|▊         | 744M/9.45G [00:10<02:15, 64.4MB/s][A
Downloading (…)l-00001-of-00002.bin:   8%|▊         | 755M/9.45G [00:11<02:18, 62.7MB/s][A
Downloading (…)l-00001-of-00002.bin:   8%|▊         | 765M/9.45G [00:11<02:19, 62.3MB/s][A
Downloading (…)l-00001-of-00002.bin:   8%|▊         | 776M/9.45G [00:11<02:06, 68.5MB/s][A
Downloading (…)l-00001-of-00002.bin:   8%|▊         | 786M/9.45G [00:11<02:06, 68.7MB/s][A
Downloading (…)l-00001-of-00002.bin:   8%|▊         | 797M/9.45G [00:11<02:54, 49.6MB/s][A
Downloading (…)l-00001-of-00002.bin:   9%|▊         | 807M/9.45G [00:11<02:32, 56.8MB/s][A
Downloading (…)l-00001-of-00002.bin:   9%|▊         | 818M/9.45G [00:12<02:21, 60.9MB/s][A
Downloading (…)l-00001-of-00002.bin:   9%|▉         | 828M/9.45G [00:12<02:10, 65.9MB/s][A
Downloading (…)l-00001-of-00002.bin:   9%|▉         | 839M/9.45G [00:12<02:06, 68.1MB/s][A
Downloading (…)l-00001-of-00002.bin:   9%|▉         | 849M/9.45G [00:12<02:06, 68.0MB/s][A
Downloading (…)l-00001-of-00002.bin:   9%|▉         | 860M/9.45G [00:12<02:18, 61.9MB/s][A
Downloading (…)l-00001-of-00002.bin:   9%|▉         | 870M/9.45G [00:12<02:07, 67.5MB/s][A
Downloading (…)l-00001-of-00002.bin:   9%|▉         | 881M/9.45G [00:13<02:00, 70.9MB/s][A
Downloading (…)l-00001-of-00002.bin:   9%|▉         | 891M/9.45G [00:13<01:57, 72.9MB/s][A
Downloading (…)l-00001-of-00002.bin:  10%|▉         | 902M/9.45G [00:13<01:50, 77.2MB/s][A
Downloading (…)l-00001-of-00002.bin:  10%|▉         | 912M/9.45G [00:13<01:56, 73.1MB/s][A
Downloading (…)l-00001-of-00002.bin:  10%|▉         | 923M/9.45G [00:13<02:04, 68.6MB/s][A
Downloading (…)l-00001-of-00002.bin:  10%|▉         | 933M/9.45G [00:13<01:58, 71.7MB/s][A
Downloading (…)l-00001-of-00002.bin:  10%|▉         | 944M/9.45G [00:13<01:57, 72.2MB/s][A
Downloading (…)l-00001-of-00002.bin:  10%|█         | 954M/9.45G [00:14<01:54, 74.2MB/s][A
Downloading (…)l-00001-of-00002.bin:  10%|█         | 965M/9.45G [00:14<01:51, 76.4MB/s][A
Downloading (…)l-00001-of-00002.bin:  10%|█         | 975M/9.45G [00:14<01:51, 75.8MB/s][A
Downloading (…)l-00001-of-00002.bin:  10%|█         | 986M/9.45G [00:14<01:53, 74.8MB/s][A
Downloading (…)l-00001-of-00002.bin:  11%|█         | 996M/9.45G [00:14<02:05, 67.3MB/s][A
Downloading (…)l-00001-of-00002.bin:  11%|█         | 1.01G/9.45G [00:14<02:18, 61.0MB/s][A
Downloading (…)l-00001-of-00002.bin:  11%|█         | 1.02G/9.45G [00:15<02:41, 52.3MB/s][A
Downloading (…)l-00001-of-00002.bin:  11%|█         | 1.03G/9.45G [00:15<02:26, 57.4MB/s][A
Downloading (…)l-00001-of-00002.bin:  11%|█         | 1.04G/9.45G [00:15<02:13, 62.9MB/s][A
Downloading (…)l-00001-of-00002.bin:  11%|█         | 1.05G/9.45G [00:15<02:22, 59.0MB/s][A
Downloading (…)l-00001-of-00002.bin:  11%|█         | 1.06G/9.45G [00:15<02:22, 59.0MB/s][A
Downloading (…)l-00001-of-00002.bin:  11%|█▏        | 1.07G/9.45G [00:15<02:15, 62.0MB/s][A
Downloading (…)l-00001-of-00002.bin:  11%|█▏        | 1.08G/9.45G [00:16<02:05, 66.5MB/s][A
Downloading (…)l-00001-of-00002.bin:  12%|█▏        | 1.09G/9.45G [00:16<01:58, 70.6MB/s][A
Downloading (…)l-00001-of-00002.bin:  12%|█▏        | 1.10G/9.45G [00:16<01:52, 74.5MB/s][A
Downloading (…)l-00001-of-00002.bin:  12%|█▏        | 1.11G/9.45G [00:16<01:50, 75.2MB/s][A
Downloading (…)l-00001-of-00002.bin:  12%|█▏        | 1.12G/9.45G [00:16<01:56, 71.7MB/s][A
Downloading (…)l-00001-of-00002.bin:  12%|█▏        | 1.13G/9.45G [00:16<01:54, 72.4MB/s][A
Downloading (…)l-00001-of-00002.bin:  12%|█▏        | 1.14G/9.45G [00:16<01:51, 74.3MB/s][A
Downloading (…)l-00001-of-00002.bin:  12%|█▏        | 1.15G/9.45G [00:16<01:50, 75.0MB/s][A
Downloading (…)l-00001-of-00002.bin:  12%|█▏        | 1.16G/9.45G [00:17<01:49, 75.7MB/s][A
Downloading (…)l-00001-of-00002.bin:  12%|█▏        | 1.17G/9.45G [00:17<01:45, 78.3MB/s][A
Downloading (…)l-00001-of-00002.bin:  13%|█▎        | 1.18G/9.45G [00:17<01:48, 76.1MB/s][A
Downloading (…)l-00001-of-00002.bin:  13%|█▎        | 1.20G/9.45G [00:17<01:46, 77.8MB/s][A
Downloading (…)l-00001-of-00002.bin:  13%|█▎        | 1.21G/9.45G [00:17<01:45, 77.9MB/s][A
Downloading (…)l-00001-of-00002.bin:  13%|█▎        | 1.22G/9.45G [00:17<01:47, 76.7MB/s][A
Downloading (…)l-00001-of-00002.bin:  13%|█▎        | 1.23G/9.45G [00:17<01:46, 77.5MB/s][A
Downloading (…)l-00001-of-00002.bin:  13%|█▎        | 1.24G/9.45G [00:18<01:57, 69.9MB/s][A
Downloading (…)l-00001-of-00002.bin:  13%|█▎        | 1.25G/9.45G [00:18<01:58, 69.1MB/s][A
Downloading (…)l-00001-of-00002.bin:  13%|█▎        | 1.26G/9.45G [00:18<01:52, 72.8MB/s][A
Downloading (…)l-00001-of-00002.bin:  13%|█▎        | 1.27G/9.45G [00:18<02:03, 66.5MB/s][A
Downloading (…)l-00001-of-00002.bin:  14%|█▎        | 1.28G/9.45G [00:18<02:29, 54.5MB/s][A
Downloading (…)l-00001-of-00002.bin:  14%|█▎        | 1.29G/9.45G [00:19<02:20, 58.1MB/s][A
Downloading (…)l-00001-of-00002.bin:  14%|█▍        | 1.30G/9.45G [00:19<02:06, 64.6MB/s][A
Downloading (…)l-00001-of-00002.bin:  14%|█▍        | 1.31G/9.45G [00:19<02:11, 61.8MB/s][A
Downloading (…)l-00001-of-00002.bin:  14%|█▍        | 1.32G/9.45G [00:19<02:05, 64.6MB/s][A
Downloading (…)l-00001-of-00002.bin:  14%|█▍        | 1.33G/9.45G [00:19<02:08, 63.2MB/s][A
Downloading (…)l-00001-of-00002.bin:  14%|█▍        | 1.34G/9.45G [00:19<02:04, 65.0MB/s][A
Downloading (…)l-00001-of-00002.bin:  14%|█▍        | 1.35G/9.45G [00:19<01:58, 68.2MB/s][A
Downloading (…)l-00001-of-00002.bin:  14%|█▍        | 1.36G/9.45G [00:20<01:56, 69.2MB/s][A
Downloading (…)l-00001-of-00002.bin:  15%|█▍        | 1.37G/9.45G [00:20<02:09, 62.3MB/s][A
Downloading (…)l-00001-of-00002.bin:  15%|█▍        | 1.38G/9.45G [00:20<02:00, 66.8MB/s][A
Downloading (…)l-00001-of-00002.bin:  15%|█▍        | 1.39G/9.45G [00:20<01:56, 69.3MB/s][A
Downloading (…)l-00001-of-00002.bin:  15%|█▍        | 1.41G/9.45G [00:20<01:52, 71.7MB/s][A
Downloading (…)l-00001-of-00002.bin:  15%|█▍        | 1.42G/9.45G [00:20<01:47, 74.9MB/s][A
Downloading (…)l-00001-of-00002.bin:  15%|█▌        | 1.43G/9.45G [00:20<01:52, 71.1MB/s][A
Downloading (…)l-00001-of-00002.bin:  15%|█▌        | 1.44G/9.45G [00:21<01:48, 73.9MB/s][A
Downloading (…)l-00001-of-00002.bin:  15%|█▌        | 1.45G/9.45G [00:21<01:46, 75.1MB/s][A
Downloading (…)l-00001-of-00002.bin:  15%|█▌        | 1.46G/9.45G [00:21<01:56, 68.8MB/s][A
Downloading (…)l-00001-of-00002.bin:  16%|█▌        | 1.47G/9.45G [00:21<01:53, 70.2MB/s][A
Downloading (…)l-00001-of-00002.bin:  16%|█▌        | 1.48G/9.45G [00:21<01:46, 74.5MB/s][A
Downloading (…)l-00001-of-00002.bin:  16%|█▌        | 1.49G/9.45G [00:21<01:46, 74.7MB/s][A
Downloading (…)l-00001-of-00002.bin:  16%|█▌        | 1.50G/9.45G [00:21<01:41, 78.1MB/s][A
Downloading (…)l-00001-of-00002.bin:  16%|█▌        | 1.51G/9.45G [00:22<01:41, 78.3MB/s][A
Downloading (…)l-00001-of-00002.bin:  16%|█▌        | 1.52G/9.45G [00:22<01:40, 78.6MB/s][A
Downloading (…)l-00001-of-00002.bin:  16%|█▌        | 1.53G/9.45G [00:22<01:38, 80.6MB/s][A
Downloading (…)l-00001-of-00002.bin:  16%|█▋        | 1.54G/9.45G [00:22<01:42, 76.9MB/s][A
Downloading (…)l-00001-of-00002.bin:  16%|█▋        | 1.55G/9.45G [00:22<01:56, 67.6MB/s][A
Downloading (…)l-00001-of-00002.bin:  17%|█▋        | 1.56G/9.45G [00:22<01:51, 70.8MB/s][A
Downloading (…)l-00001-of-00002.bin:  17%|█▋        | 1.57G/9.45G [00:22<01:45, 74.9MB/s][A
Downloading (…)l-00001-of-00002.bin:  17%|█▋        | 1.58G/9.45G [00:23<01:44, 75.2MB/s][A
Downloading (…)l-00001-of-00002.bin:  17%|█▋        | 1.59G/9.45G [00:23<01:46, 73.7MB/s][A
Downloading (…)l-00001-of-00002.bin:  17%|█▋        | 1.60G/9.45G [00:23<01:46, 73.9MB/s][A
Downloading (…)l-00001-of-00002.bin:  17%|█▋        | 1.61G/9.45G [00:23<01:54, 68.5MB/s][A
Downloading (…)l-00001-of-00002.bin:  17%|█▋        | 1.63G/9.45G [00:23<01:50, 70.9MB/s][A
Downloading (…)l-00001-of-00002.bin:  17%|█▋        | 1.64G/9.45G [00:23<01:45, 73.8MB/s][A
Downloading (…)l-00001-of-00002.bin:  17%|█▋        | 1.65G/9.45G [00:23<01:46, 73.4MB/s][A
Downloading (…)l-00001-of-00002.bin:  18%|█▊        | 1.66G/9.45G [00:24<02:00, 64.5MB/s][A
Downloading (…)l-00001-of-00002.bin:  18%|█▊        | 1.67G/9.45G [00:24<02:10, 59.5MB/s][A
Downloading (…)l-00001-of-00002.bin:  18%|█▊        | 1.68G/9.45G [00:24<02:01, 64.0MB/s][A
Downloading (…)l-00001-of-00002.bin:  18%|█▊        | 1.69G/9.45G [00:24<01:53, 68.3MB/s][A
Downloading (…)l-00001-of-00002.bin:  18%|█▊        | 1.70G/9.45G [00:24<01:51, 69.5MB/s][A
Downloading (…)l-00001-of-00002.bin:  18%|█▊        | 1.71G/9.45G [00:24<01:44, 74.3MB/s][A
Downloading (…)l-00001-of-00002.bin:  18%|█▊        | 1.72G/9.45G [00:25<01:46, 72.8MB/s][A
Downloading (…)l-00001-of-00002.bin:  18%|█▊        | 1.73G/9.45G [00:25<01:39, 77.3MB/s][A
Downloading (…)l-00001-of-00002.bin:  18%|█▊        | 1.74G/9.45G [00:25<01:38, 78.2MB/s][A
Downloading (…)l-00001-of-00002.bin:  19%|█▊        | 1.75G/9.45G [00:25<01:38, 78.0MB/s][A
Downloading (…)l-00001-of-00002.bin:  19%|█▊        | 1.76G/9.45G [00:25<01:38, 77.8MB/s][A
Downloading (…)l-00001-of-00002.bin:  19%|█▉        | 1.77G/9.45G [00:25<01:41, 75.5MB/s][A
Downloading (…)l-00001-of-00002.bin:  19%|█▉        | 1.78G/9.45G [00:25<01:40, 76.0MB/s][A
Downloading (…)l-00001-of-00002.bin:  19%|█▉        | 1.79G/9.45G [00:25<01:38, 78.0MB/s][A
Downloading (…)l-00001-of-00002.bin:  19%|█▉        | 1.80G/9.45G [00:26<02:03, 61.9MB/s][A
Downloading (…)l-00001-of-00002.bin:  19%|█▉        | 1.81G/9.45G [00:26<02:06, 60.2MB/s][A
Downloading (…)l-00001-of-00002.bin:  19%|█▉        | 1.82G/9.45G [00:26<01:55, 65.9MB/s][A
Downloading (…)l-00001-of-00002.bin:  19%|█▉        | 1.84G/9.45G [00:26<01:53, 67.1MB/s][A
Downloading (…)l-00001-of-00002.bin:  20%|█▉        | 1.85G/9.45G [00:26<01:53, 67.1MB/s][A
Downloading (…)l-00001-of-00002.bin:  20%|█▉        | 1.86G/9.45G [00:27<02:18, 54.7MB/s][A
Downloading (…)l-00001-of-00002.bin:  20%|█▉        | 1.87G/9.45G [00:27<02:07, 59.6MB/s][A
Downloading (…)l-00001-of-00002.bin:  20%|█▉        | 1.88G/9.45G [00:27<02:12, 57.1MB/s][A
Downloading (…)l-00001-of-00002.bin:  20%|█▉        | 1.89G/9.45G [00:27<02:12, 56.9MB/s][A
Downloading (…)l-00001-of-00002.bin:  20%|██        | 1.90G/9.45G [00:27<02:09, 58.1MB/s][A
Downloading (…)l-00001-of-00002.bin:  20%|██        | 1.91G/9.45G [00:27<01:56, 64.7MB/s][A
Downloading (…)l-00001-of-00002.bin:  20%|██        | 1.92G/9.45G [00:28<01:49, 68.7MB/s][A
Downloading (…)l-00001-of-00002.bin:  20%|██        | 1.93G/9.45G [00:28<01:45, 71.4MB/s][A
Downloading (…)l-00001-of-00002.bin:  21%|██        | 1.94G/9.45G [00:28<01:45, 71.5MB/s][A
Downloading (…)l-00001-of-00002.bin:  21%|██        | 1.95G/9.45G [00:28<01:47, 70.0MB/s][A
Downloading (…)l-00001-of-00002.bin:  21%|██        | 1.96G/9.45G [00:28<01:46, 70.5MB/s][A
Downloading (…)l-00001-of-00002.bin:  21%|██        | 1.97G/9.45G [00:28<01:45, 70.6MB/s][A
Downloading (…)l-00001-of-00002.bin:  21%|██        | 1.98G/9.45G [00:28<01:44, 71.5MB/s][A
Downloading (…)l-00001-of-00002.bin:  21%|██        | 1.99G/9.45G [00:29<01:42, 72.8MB/s][A
Downloading (…)l-00001-of-00002.bin:  21%|██        | 2.00G/9.45G [00:29<01:40, 74.2MB/s][A
Downloading (…)l-00001-of-00002.bin:  21%|██▏       | 2.01G/9.45G [00:29<01:44, 71.3MB/s][A
Downloading (…)l-00001-of-00002.bin:  21%|██▏       | 2.02G/9.45G [00:29<01:41, 73.0MB/s][A
Downloading (…)l-00001-of-00002.bin:  22%|██▏       | 2.03G/9.45G [00:29<01:44, 70.7MB/s][A
Downloading (…)l-00001-of-00002.bin:  22%|██▏       | 2.04G/9.45G [00:29<01:45, 69.9MB/s][A
Downloading (…)l-00001-of-00002.bin:  22%|██▏       | 2.06G/9.45G [00:29<01:43, 71.4MB/s][A
Downloading (…)l-00001-of-00002.bin:  22%|██▏       | 2.07G/9.45G [00:30<01:42, 71.7MB/s][A
Downloading (…)l-00001-of-00002.bin:  22%|██▏       | 2.08G/9.45G [00:30<01:43, 71.3MB/s][A
Downloading (…)l-00001-of-00002.bin:  22%|██▏       | 2.09G/9.45G [00:30<01:48, 67.6MB/s][A
Downloading (…)l-00001-of-00002.bin:  22%|██▏       | 2.10G/9.45G [00:30<01:42, 71.4MB/s][A
Downloading (…)l-00001-of-00002.bin:  22%|██▏       | 2.11G/9.45G [00:30<01:45, 69.5MB/s][A
Downloading (…)l-00001-of-00002.bin:  22%|██▏       | 2.12G/9.45G [00:30<01:42, 71.3MB/s][A
Downloading (…)l-00001-of-00002.bin:  23%|██▎       | 2.13G/9.45G [00:30<01:41, 72.2MB/s][A
Downloading (…)l-00001-of-00002.bin:  23%|██▎       | 2.14G/9.45G [00:31<01:36, 75.9MB/s][A
Downloading (…)l-00001-of-00002.bin:  23%|██▎       | 2.15G/9.45G [00:31<01:55, 63.5MB/s][A
Downloading (…)l-00001-of-00002.bin:  23%|██▎       | 2.16G/9.45G [00:31<02:06, 57.5MB/s][A
Downloading (…)l-00001-of-00002.bin:  23%|██▎       | 2.17G/9.45G [00:31<01:55, 63.2MB/s][A
Downloading (…)l-00001-of-00002.bin:  23%|██▎       | 2.18G/9.45G [00:31<02:19, 52.1MB/s][A
Downloading (…)l-00001-of-00002.bin:  23%|██▎       | 2.19G/9.45G [00:32<02:04, 58.5MB/s][A
Downloading (…)l-00001-of-00002.bin:  23%|██▎       | 2.20G/9.45G [00:32<01:55, 62.7MB/s][A
Downloading (…)l-00001-of-00002.bin:  23%|██▎       | 2.21G/9.45G [00:32<02:14, 54.0MB/s][A
Downloading (…)l-00001-of-00002.bin:  24%|██▎       | 2.22G/9.45G [00:32<02:08, 56.3MB/s][A
Downloading (…)l-00001-of-00002.bin:  24%|██▎       | 2.23G/9.45G [00:32<01:59, 60.6MB/s][A
Downloading (…)l-00001-of-00002.bin:  24%|██▎       | 2.24G/9.45G [00:32<01:51, 64.6MB/s][A
Downloading (…)l-00001-of-00002.bin:  24%|██▍       | 2.25G/9.45G [00:33<01:51, 64.5MB/s][A
Downloading (…)l-00001-of-00002.bin:  24%|██▍       | 2.26G/9.45G [00:33<01:53, 63.2MB/s][A
Downloading (…)l-00001-of-00002.bin:  24%|██▍       | 2.28G/9.45G [00:33<01:47, 66.7MB/s][A
Downloading (…)l-00001-of-00002.bin:  24%|██▍       | 2.29G/9.45G [00:33<01:52, 63.5MB/s][A
Downloading (…)l-00001-of-00002.bin:  24%|██▍       | 2.30G/9.45G [00:33<01:44, 68.4MB/s][A
Downloading (…)l-00001-of-00002.bin:  24%|██▍       | 2.31G/9.45G [00:33<01:38, 72.2MB/s][A
Downloading (…)l-00001-of-00002.bin:  25%|██▍       | 2.32G/9.45G [00:33<01:33, 76.0MB/s][A
Downloading (…)l-00001-of-00002.bin:  25%|██▍       | 2.33G/9.45G [00:34<01:35, 74.8MB/s][A
Downloading (…)l-00001-of-00002.bin:  25%|██▍       | 2.34G/9.45G [00:34<01:43, 68.6MB/s][A
Downloading (…)l-00001-of-00002.bin:  25%|██▍       | 2.35G/9.45G [00:34<01:55, 61.7MB/s][A
Downloading (…)l-00001-of-00002.bin:  25%|██▍       | 2.36G/9.45G [00:34<01:46, 66.6MB/s][A
Downloading (…)l-00001-of-00002.bin:  25%|██▌       | 2.37G/9.45G [00:34<01:49, 64.7MB/s][A
Downloading (…)l-00001-of-00002.bin:  25%|██▌       | 2.38G/9.45G [00:34<01:48, 65.1MB/s][A
Downloading (…)l-00001-of-00002.bin:  25%|██▌       | 2.39G/9.45G [00:35<01:41, 69.3MB/s][A
Downloading (…)l-00001-of-00002.bin:  25%|██▌       | 2.40G/9.45G [00:35<01:43, 68.1MB/s][A
Downloading (…)l-00001-of-00002.bin:  26%|██▌       | 2.41G/9.45G [00:35<01:55, 60.8MB/s][A
Downloading (…)l-00001-of-00002.bin:  26%|██▌       | 2.42G/9.45G [00:35<01:59, 59.0MB/s][A
Downloading (…)l-00001-of-00002.bin:  26%|██▌       | 2.43G/9.45G [00:35<01:48, 64.5MB/s][A
Downloading (…)l-00001-of-00002.bin:  26%|██▌       | 2.44G/9.45G [00:35<01:42, 68.1MB/s][A
Downloading (…)l-00001-of-00002.bin:  26%|██▌       | 2.45G/9.45G [00:36<01:41, 69.0MB/s][A
Downloading (…)l-00001-of-00002.bin:  26%|██▌       | 2.46G/9.45G [00:36<01:37, 71.3MB/s][A
Downloading (…)l-00001-of-00002.bin:  26%|██▌       | 2.47G/9.45G [00:36<01:54, 61.2MB/s][A
Downloading (…)l-00001-of-00002.bin:  26%|██▋       | 2.49G/9.45G [00:36<01:45, 65.7MB/s][A
Downloading (…)l-00001-of-00002.bin:  26%|██▋       | 2.50G/9.45G [00:36<01:39, 70.0MB/s][A
Downloading (…)l-00001-of-00002.bin:  27%|██▋       | 2.51G/9.45G [00:36<01:57, 59.1MB/s][A
Downloading (…)l-00001-of-00002.bin:  27%|██▋       | 2.52G/9.45G [00:37<01:48, 64.0MB/s][A
Downloading (…)l-00001-of-00002.bin:  27%|██▋       | 2.53G/9.45G [00:37<01:49, 63.0MB/s][A
Downloading (…)l-00001-of-00002.bin:  27%|██▋       | 2.54G/9.45G [00:37<01:43, 66.5MB/s][A
Downloading (…)l-00001-of-00002.bin:  27%|██▋       | 2.55G/9.45G [00:37<01:46, 64.6MB/s][A
Downloading (…)l-00001-of-00002.bin:  27%|██▋       | 2.56G/9.45G [00:37<02:03, 56.0MB/s][Aslurmstepd: error: *** JOB 8462187 ON notch372 CANCELLED AT 2023-09-05T21:20:23 ***
