mkdir: cannot create directory ‘/scratch/general/vast/u1283221/huggingface_cache’: File exists
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 2240.55it/s]
Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 633.29it/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 100 examples [00:00, 1739.98 examples/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:03<00:00,  3.84s/it]100%|██████████| 1/1 [00:03<00:00,  3.84s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 8246.93 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 42.85ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.29it/s]100%|██████████| 1/1 [00:00<00:00,  1.29it/s]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 8154.25 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 680.45ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.26it/s]100%|██████████| 1/1 [00:00<00:00,  1.26it/s]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 8238.34 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 885.62ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.29it/s]100%|██████████| 1/1 [00:00<00:00,  1.29it/s]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 1692.18 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1136.98ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.26it/s]100%|██████████| 1/1 [00:00<00:00,  1.26it/s]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 9473.30 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1201.46ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.29it/s]100%|██████████| 1/1 [00:00<00:00,  1.29it/s]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 9785.83 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 677.16ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.28it/s]100%|██████████| 1/1 [00:00<00:00,  1.28it/s]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 7262.74 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1180.50ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.28it/s]100%|██████████| 1/1 [00:00<00:00,  1.28it/s]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 8308.68 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 767.20ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.29it/s]100%|██████████| 1/1 [00:00<00:00,  1.29it/s]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 10462.22 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1255.03ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.28it/s]100%|██████████| 1/1 [00:00<00:00,  1.28it/s]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 10433.07 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1247.19ba/s]
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 16131.94it/s]
Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 677.05it/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 100 examples [00:00, 12602.70 examples/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:01<00:00,  1.21s/it]100%|██████████| 1/1 [00:01<00:00,  1.21s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 7428.02 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 349.76ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:01<00:00,  1.22s/it]100%|██████████| 1/1 [00:01<00:00,  1.22s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 9580.41 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1093.69ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.05s/it]100%|██████████| 1/1 [00:02<00:00,  2.05s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 9377.78 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 880.60ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.05s/it]100%|██████████| 1/1 [00:02<00:00,  2.05s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 9111.91 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 797.40ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:01<00:00,  1.08s/it]100%|██████████| 1/1 [00:01<00:00,  1.08s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 9872.90 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 767.48ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:01<00:00,  1.16s/it]100%|██████████| 1/1 [00:01<00:00,  1.16s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 9116.27 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 859.31ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.22it/s]100%|██████████| 1/1 [00:00<00:00,  1.22it/s]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 9155.47 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 840.88ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.06s/it]100%|██████████| 1/1 [00:02<00:00,  2.06s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 9723.22 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 694.19ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:01<00:00,  1.09s/it]100%|██████████| 1/1 [00:01<00:00,  1.09s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 9650.29 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1246.08ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:01<00:00,  1.09s/it]100%|██████████| 1/1 [00:01<00:00,  1.09s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 10277.13 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1264.49ba/s]
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 17050.02it/s]
Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 460.61it/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 10000 examples [00:00, 556502.54 examples/s]
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:03<01:15,  3.95s/it] 10%|█         | 2/20 [00:07<01:07,  3.75s/it] 15%|█▌        | 3/20 [00:11<01:02,  3.67s/it] 20%|██        | 4/20 [00:14<00:58,  3.64s/it] 25%|██▌       | 5/20 [00:18<00:54,  3.62s/it] 30%|███       | 6/20 [00:21<00:50,  3.61s/it] 35%|███▌      | 7/20 [00:25<00:46,  3.61s/it] 40%|████      | 8/20 [00:29<00:43,  3.62s/it] 45%|████▌     | 9/20 [00:32<00:39,  3.63s/it] 50%|█████     | 10/20 [00:36<00:36,  3.63s/it] 55%|█████▌    | 11/20 [00:40<00:32,  3.64s/it] 60%|██████    | 12/20 [00:43<00:29,  3.66s/it] 65%|██████▌   | 13/20 [00:47<00:25,  3.67s/it] 70%|███████   | 14/20 [00:51<00:22,  3.68s/it] 75%|███████▌  | 15/20 [00:54<00:18,  3.68s/it] 80%|████████  | 16/20 [00:58<00:14,  3.69s/it] 85%|████████▌ | 17/20 [01:02<00:11,  3.70s/it] 90%|█████████ | 18/20 [01:06<00:07,  3.71s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.72s/it]100%|██████████| 20/20 [01:13<00:00,  3.72s/it]100%|██████████| 20/20 [01:13<00:00,  3.68s/it]
Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Map:  61%|██████    | 6121/10000 [00:00<00:00, 59583.63 examples/s]Map: 100%|██████████| 10000/10000 [00:00<00:00, 55356.68 examples/s]
Creating CSV from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 514.20ba/s]
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:03<01:10,  3.69s/it] 10%|█         | 2/20 [00:07<01:05,  3.63s/it] 15%|█▌        | 3/20 [00:10<01:01,  3.61s/it] 20%|██        | 4/20 [00:14<00:57,  3.59s/it] 25%|██▌       | 5/20 [00:18<00:53,  3.59s/it] 30%|███       | 6/20 [00:21<00:50,  3.59s/it] 35%|███▌      | 7/20 [00:25<00:46,  3.59s/it] 40%|████      | 8/20 [00:28<00:43,  3.60s/it] 45%|████▌     | 9/20 [00:32<00:39,  3.61s/it] 50%|█████     | 10/20 [00:36<00:36,  3.63s/it] 55%|█████▌    | 11/20 [00:39<00:32,  3.63s/it] 60%|██████    | 12/20 [00:43<00:29,  3.65s/it] 65%|██████▌   | 13/20 [00:47<00:25,  3.67s/it] 70%|███████   | 14/20 [00:50<00:22,  3.68s/it] 75%|███████▌  | 15/20 [00:54<00:18,  3.68s/it] 80%|████████  | 16/20 [00:58<00:14,  3.69s/it] 85%|████████▌ | 17/20 [01:01<00:11,  3.70s/it] 90%|█████████ | 18/20 [01:05<00:07,  3.71s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.71s/it]100%|██████████| 20/20 [01:13<00:00,  3.72s/it]100%|██████████| 20/20 [01:13<00:00,  3.66s/it]
Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Map:  57%|█████▋    | 5672/10000 [00:00<00:00, 56508.13 examples/s]Map: 100%|██████████| 10000/10000 [00:00<00:00, 51486.03 examples/s]
Creating CSV from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 607.48ba/s]
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:03<01:08,  3.62s/it] 10%|█         | 2/20 [00:07<01:04,  3.60s/it] 15%|█▌        | 3/20 [00:10<01:01,  3.59s/it] 20%|██        | 4/20 [00:14<00:57,  3.58s/it] 25%|██▌       | 5/20 [00:17<00:53,  3.58s/it] 30%|███       | 6/20 [00:21<00:50,  3.58s/it] 35%|███▌      | 7/20 [00:25<00:46,  3.59s/it] 40%|████      | 8/20 [00:28<00:43,  3.60s/it] 45%|████▌     | 9/20 [00:32<00:39,  3.61s/it] 50%|█████     | 10/20 [00:36<00:36,  3.63s/it] 55%|█████▌    | 11/20 [00:39<00:32,  3.63s/it] 60%|██████    | 12/20 [00:43<00:29,  3.65s/it] 65%|██████▌   | 13/20 [00:47<00:25,  3.66s/it] 70%|███████   | 14/20 [00:50<00:22,  3.68s/it] 75%|███████▌  | 15/20 [00:54<00:18,  3.68s/it] 80%|████████  | 16/20 [00:58<00:14,  3.69s/it] 85%|████████▌ | 17/20 [01:01<00:11,  3.70s/it] 90%|█████████ | 18/20 [01:05<00:07,  3.70s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.71s/it]100%|██████████| 20/20 [01:13<00:00,  3.72s/it]100%|██████████| 20/20 [01:13<00:00,  3.65s/it]
Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Map:  58%|█████▊    | 5766/10000 [00:00<00:00, 57461.00 examples/s]Map: 100%|██████████| 10000/10000 [00:00<00:00, 51929.37 examples/s]
Creating CSV from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 614.06ba/s]
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:03<01:08,  3.61s/it] 10%|█         | 2/20 [00:07<01:04,  3.60s/it] 15%|█▌        | 3/20 [00:10<01:01,  3.59s/it] 20%|██        | 4/20 [00:14<00:57,  3.58s/it] 25%|██▌       | 5/20 [00:17<00:53,  3.58s/it] 30%|███       | 6/20 [00:21<00:50,  3.60s/it] 35%|███▌      | 7/20 [00:25<00:46,  3.60s/it] 40%|████      | 8/20 [00:28<00:43,  3.61s/it] 45%|████▌     | 9/20 [00:32<00:39,  3.61s/it] 50%|█████     | 10/20 [00:36<00:36,  3.63s/it] 55%|█████▌    | 11/20 [00:39<00:32,  3.63s/it] 60%|██████    | 12/20 [00:43<00:29,  3.65s/it] 65%|██████▌   | 13/20 [00:47<00:25,  3.66s/it] 70%|███████   | 14/20 [00:50<00:22,  3.68s/it] 75%|███████▌  | 15/20 [00:54<00:18,  3.69s/it] 80%|████████  | 16/20 [00:58<00:14,  3.69s/it] 85%|████████▌ | 17/20 [01:01<00:11,  3.70s/it] 90%|█████████ | 18/20 [01:05<00:07,  3.71s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.71s/it]100%|██████████| 20/20 [01:13<00:00,  3.73s/it]100%|██████████| 20/20 [01:13<00:00,  3.66s/it]
Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Map:  57%|█████▋    | 5707/10000 [00:00<00:00, 56861.41 examples/s]Map: 100%|██████████| 10000/10000 [00:00<00:00, 51733.76 examples/s]
Creating CSV from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 592.81ba/s]
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:03<01:08,  3.60s/it] 10%|█         | 2/20 [00:07<01:04,  3.60s/it] 15%|█▌        | 3/20 [00:10<01:01,  3.59s/it] 20%|██        | 4/20 [00:14<00:57,  3.59s/it] 25%|██▌       | 5/20 [00:17<00:53,  3.58s/it] 30%|███       | 6/20 [00:21<00:50,  3.58s/it] 35%|███▌      | 7/20 [00:25<00:46,  3.59s/it] 40%|████      | 8/20 [00:28<00:43,  3.60s/it] 45%|████▌     | 9/20 [00:32<00:39,  3.61s/it] 50%|█████     | 10/20 [00:36<00:36,  3.63s/it] 55%|█████▌    | 11/20 [00:39<00:32,  3.63s/it] 60%|██████    | 12/20 [00:43<00:29,  3.65s/it] 65%|██████▌   | 13/20 [00:47<00:25,  3.69s/it] 70%|███████   | 14/20 [00:50<00:22,  3.69s/it] 75%|███████▌  | 15/20 [00:54<00:18,  3.70s/it] 80%|████████  | 16/20 [00:58<00:14,  3.70s/it] 85%|████████▌ | 17/20 [01:01<00:11,  3.70s/it] 90%|█████████ | 18/20 [01:05<00:07,  3.72s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.72s/it]100%|██████████| 20/20 [01:13<00:00,  3.73s/it]100%|██████████| 20/20 [01:13<00:00,  3.66s/it]
Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Map:  57%|█████▋    | 5732/10000 [00:00<00:00, 57109.01 examples/s]Map: 100%|██████████| 10000/10000 [00:00<00:00, 49376.53 examples/s]
Creating CSV from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 624.80ba/s]
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:03<01:08,  3.63s/it] 10%|█         | 2/20 [00:07<01:04,  3.60s/it] 15%|█▌        | 3/20 [00:10<01:01,  3.59s/it] 20%|██        | 4/20 [00:14<00:57,  3.59s/it] 25%|██▌       | 5/20 [00:17<00:53,  3.58s/it] 30%|███       | 6/20 [00:21<00:50,  3.58s/it] 35%|███▌      | 7/20 [00:25<00:46,  3.59s/it] 40%|████      | 8/20 [00:28<00:43,  3.60s/it] 45%|████▌     | 9/20 [00:32<00:39,  3.61s/it] 50%|█████     | 10/20 [00:36<00:36,  3.63s/it] 55%|█████▌    | 11/20 [00:39<00:32,  3.63s/it] 60%|██████    | 12/20 [00:43<00:29,  3.67s/it] 65%|██████▌   | 13/20 [00:47<00:25,  3.68s/it] 70%|███████   | 14/20 [00:50<00:22,  3.69s/it] 75%|███████▌  | 15/20 [00:54<00:18,  3.70s/it] 80%|████████  | 16/20 [00:58<00:14,  3.70s/it] 85%|████████▌ | 17/20 [01:02<00:11,  3.71s/it] 90%|█████████ | 18/20 [01:05<00:07,  3.71s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.71s/it]100%|██████████| 20/20 [01:13<00:00,  3.72s/it]100%|██████████| 20/20 [01:13<00:00,  3.66s/it]
Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Map:  56%|█████▌    | 5560/10000 [00:00<00:00, 55404.55 examples/s]Map: 100%|██████████| 10000/10000 [00:00<00:00, 49870.92 examples/s]
Creating CSV from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 620.36ba/s]
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:03<01:08,  3.61s/it] 10%|█         | 2/20 [00:07<01:04,  3.60s/it] 15%|█▌        | 3/20 [00:10<01:01,  3.59s/it] 20%|██        | 4/20 [00:14<00:57,  3.58s/it] 25%|██▌       | 5/20 [00:17<00:53,  3.58s/it] 30%|███       | 6/20 [00:21<00:50,  3.58s/it] 35%|███▌      | 7/20 [00:25<00:46,  3.58s/it] 40%|████      | 8/20 [00:28<00:43,  3.59s/it] 45%|████▌     | 9/20 [00:32<00:39,  3.60s/it] 50%|█████     | 10/20 [00:35<00:36,  3.62s/it] 55%|█████▌    | 11/20 [00:39<00:32,  3.63s/it] 60%|██████    | 12/20 [00:43<00:29,  3.65s/it] 65%|██████▌   | 13/20 [00:47<00:25,  3.67s/it] 70%|███████   | 14/20 [00:50<00:22,  3.68s/it] 75%|███████▌  | 15/20 [00:54<00:18,  3.69s/it] 80%|████████  | 16/20 [00:58<00:14,  3.70s/it] 85%|████████▌ | 17/20 [01:01<00:11,  3.70s/it] 90%|█████████ | 18/20 [01:05<00:07,  3.71s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.72s/it]100%|██████████| 20/20 [01:13<00:00,  3.73s/it]100%|██████████| 20/20 [01:13<00:00,  3.66s/it]
Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Map:  57%|█████▋    | 5701/10000 [00:00<00:00, 56813.24 examples/s]Map: 100%|██████████| 10000/10000 [00:00<00:00, 51371.89 examples/s]
Creating CSV from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 624.40ba/s]
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:03<01:08,  3.62s/it] 10%|█         | 2/20 [00:07<01:04,  3.60s/it] 15%|█▌        | 3/20 [00:10<01:01,  3.60s/it] 20%|██        | 4/20 [00:14<00:57,  3.59s/it] 25%|██▌       | 5/20 [00:17<00:53,  3.58s/it] 30%|███       | 6/20 [00:21<00:50,  3.59s/it] 35%|███▌      | 7/20 [00:25<00:46,  3.59s/it] 40%|████      | 8/20 [00:28<00:43,  3.60s/it] 45%|████▌     | 9/20 [00:32<00:39,  3.61s/it] 50%|█████     | 10/20 [00:36<00:36,  3.63s/it] 55%|█████▌    | 11/20 [00:39<00:32,  3.63s/it] 60%|██████    | 12/20 [00:43<00:29,  3.65s/it] 65%|██████▌   | 13/20 [00:47<00:25,  3.67s/it] 70%|███████   | 14/20 [00:50<00:22,  3.68s/it] 75%|███████▌  | 15/20 [00:54<00:18,  3.69s/it] 80%|████████  | 16/20 [00:58<00:14,  3.69s/it] 85%|████████▌ | 17/20 [01:01<00:11,  3.70s/it] 90%|█████████ | 18/20 [01:05<00:07,  3.73s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.73s/it]100%|██████████| 20/20 [01:13<00:00,  3.73s/it]100%|██████████| 20/20 [01:13<00:00,  3.66s/it]
Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Map:  57%|█████▋    | 5666/10000 [00:00<00:00, 56453.58 examples/s]Map: 100%|██████████| 10000/10000 [00:00<00:00, 51224.14 examples/s]
Creating CSV from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 609.34ba/s]
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:03<01:08,  3.61s/it] 10%|█         | 2/20 [00:07<01:04,  3.60s/it] 15%|█▌        | 3/20 [00:10<01:01,  3.59s/it] 20%|██        | 4/20 [00:14<00:57,  3.59s/it] 25%|██▌       | 5/20 [00:17<00:53,  3.58s/it] 30%|███       | 6/20 [00:21<00:50,  3.58s/it] 35%|███▌      | 7/20 [00:25<00:46,  3.59s/it] 40%|████      | 8/20 [00:28<00:43,  3.60s/it] 45%|████▌     | 9/20 [00:32<00:39,  3.61s/it] 50%|█████     | 10/20 [00:36<00:36,  3.62s/it] 55%|█████▌    | 11/20 [00:39<00:32,  3.63s/it] 60%|██████    | 12/20 [00:43<00:29,  3.65s/it] 65%|██████▌   | 13/20 [00:47<00:25,  3.66s/it] 70%|███████   | 14/20 [00:50<00:22,  3.67s/it] 75%|███████▌  | 15/20 [00:54<00:18,  3.69s/it] 80%|████████  | 16/20 [00:58<00:14,  3.69s/it] 85%|████████▌ | 17/20 [01:01<00:11,  3.70s/it] 90%|█████████ | 18/20 [01:05<00:07,  3.70s/it] 95%|█████████▌| 19/20 [01:09<00:03,  3.71s/it]100%|██████████| 20/20 [01:13<00:00,  3.72s/it]100%|██████████| 20/20 [01:13<00:00,  3.65s/it]
Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Map:  58%|█████▊    | 5772/10000 [00:00<00:00, 57519.83 examples/s]Map: 100%|██████████| 10000/10000 [00:00<00:00, 52052.07 examples/s]
Creating CSV from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 611.20ba/s]
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:03<01:09,  3.64s/it] 10%|█         | 2/20 [00:07<01:05,  3.63s/it] 15%|█▌        | 3/20 [00:10<01:01,  3.62s/it] 20%|██        | 4/20 [00:14<00:57,  3.62s/it] 25%|██▌       | 5/20 [00:18<00:54,  3.62s/it] 30%|███       | 6/20 [00:21<00:50,  3.62s/it] 35%|███▌      | 7/20 [00:25<00:47,  3.65s/it] 40%|████      | 8/20 [00:29<00:43,  3.65s/it] 45%|████▌     | 9/20 [00:32<00:40,  3.65s/it] 50%|█████     | 10/20 [00:36<00:36,  3.67s/it] 55%|█████▌    | 11/20 [00:40<00:33,  3.67s/it] 60%|██████    | 12/20 [00:43<00:29,  3.69s/it] 65%|██████▌   | 13/20 [00:47<00:25,  3.70s/it] 70%|███████   | 14/20 [00:51<00:22,  3.71s/it] 75%|███████▌  | 15/20 [00:55<00:18,  3.72s/it] 80%|████████  | 16/20 [00:58<00:14,  3.72s/it] 85%|████████▌ | 17/20 [01:02<00:11,  3.73s/it] 90%|█████████ | 18/20 [01:06<00:07,  3.74s/it] 95%|█████████▌| 19/20 [01:10<00:03,  3.74s/it]100%|██████████| 20/20 [01:13<00:00,  3.75s/it]100%|██████████| 20/20 [01:13<00:00,  3.69s/it]
Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Map:  54%|█████▎    | 5360/10000 [00:00<00:00, 53400.16 examples/s]Map: 100%|██████████| 10000/10000 [00:00<00:00, 49498.66 examples/s]
Creating CSV from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 588.14ba/s]
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 13400.33it/s]
Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 202.81it/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 10000 examples [00:00, 437599.53 examples/s]
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:07<02:30,  7.90s/it] 10%|█         | 2/20 [00:13<01:57,  6.55s/it] 15%|█▌        | 3/20 [00:20<01:56,  6.88s/it] 20%|██        | 4/20 [00:28<01:54,  7.14s/it] 25%|██▌       | 5/20 [00:34<01:43,  6.88s/it] 30%|███       | 6/20 [00:41<01:35,  6.86s/it] 35%|███▌      | 7/20 [00:47<01:23,  6.44s/it] 40%|████      | 8/20 [00:51<01:09,  5.77s/it] 45%|████▌     | 9/20 [00:55<00:56,  5.16s/it] 50%|█████     | 10/20 [00:59<00:47,  4.74s/it] 55%|█████▌    | 11/20 [01:04<00:45,  5.05s/it] 60%|██████    | 12/20 [01:09<00:39,  4.98s/it] 65%|██████▌   | 13/20 [01:14<00:35,  5.09s/it] 70%|███████   | 14/20 [01:18<00:28,  4.69s/it] 75%|███████▌  | 15/20 [01:22<00:22,  4.42s/it] 80%|████████  | 16/20 [01:26<00:16,  4.22s/it] 85%|████████▌ | 17/20 [01:30<00:12,  4.08s/it] 90%|█████████ | 18/20 [01:33<00:07,  3.99s/it] 95%|█████████▌| 19/20 [01:37<00:03,  3.92s/it]100%|██████████| 20/20 [01:41<00:00,  4.04s/it]100%|██████████| 20/20 [01:41<00:00,  5.10s/it]
Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Map:  57%|█████▋    | 5733/10000 [00:00<00:00, 57093.88 examples/s]Map: 100%|██████████| 10000/10000 [00:00<00:00, 49669.71 examples/s]
Creating CSV from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 441.70ba/s]
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:48,  5.69s/it] 10%|█         | 2/20 [00:11<01:41,  5.66s/it] 15%|█▌        | 3/20 [00:18<01:49,  6.44s/it] 20%|██        | 4/20 [00:26<01:50,  6.89s/it] 25%|██▌       | 5/20 [00:33<01:45,  7.03s/it] 30%|███       | 6/20 [00:39<01:31,  6.57s/it] 35%|███▌      | 7/20 [00:42<01:13,  5.65s/it] 40%|████      | 8/20 [00:49<01:12,  6.08s/it] 45%|████▌     | 9/20 [00:55<01:03,  5.76s/it] 50%|█████     | 10/20 [00:59<00:54,  5.48s/it] 55%|█████▌    | 11/20 [01:07<00:54,  6.03s/it] 60%|██████    | 12/20 [01:12<00:46,  5.82s/it] 65%|██████▌   | 13/20 [01:17<00:38,  5.53s/it] 70%|███████   | 14/20 [01:21<00:29,  5.00s/it] 75%|███████▌  | 15/20 [01:28<00:28,  5.79s/it] 80%|████████  | 16/20 [01:32<00:20,  5.19s/it] 85%|████████▌ | 17/20 [01:36<00:14,  4.76s/it] 90%|█████████ | 18/20 [01:40<00:08,  4.46s/it] 95%|█████████▌| 19/20 [01:43<00:04,  4.25s/it]100%|██████████| 20/20 [01:49<00:00,  4.57s/it]100%|██████████| 20/20 [01:49<00:00,  5.46s/it]
Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Map:  52%|█████▏    | 5240/10000 [00:00<00:00, 52200.18 examples/s]Map: 100%|██████████| 10000/10000 [00:00<00:00, 48752.04 examples/s]
Creating CSV from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 562.52ba/s]
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:07<02:18,  7.28s/it] 10%|█         | 2/20 [00:12<01:53,  6.33s/it] 15%|█▌        | 3/20 [00:20<01:55,  6.79s/it] 20%|██        | 4/20 [00:27<01:51,  6.98s/it] 25%|██▌       | 5/20 [00:34<01:43,  6.91s/it] 30%|███       | 6/20 [00:39<01:30,  6.46s/it] 35%|███▌      | 7/20 [00:45<01:19,  6.10s/it] 40%|████      | 8/20 [00:49<01:04,  5.37s/it] 45%|████▌     | 9/20 [00:52<00:53,  4.87s/it] 50%|█████     | 10/20 [01:00<00:56,  5.62s/it] 55%|█████▌    | 11/20 [01:05<00:50,  5.64s/it] 60%|██████    | 12/20 [01:09<00:40,  5.07s/it] 65%|██████▌   | 13/20 [01:13<00:32,  4.69s/it] 70%|███████   | 14/20 [01:17<00:26,  4.40s/it] 75%|███████▌  | 15/20 [01:20<00:21,  4.22s/it] 80%|████████  | 16/20 [01:24<00:16,  4.10s/it] 85%|████████▌ | 17/20 [01:28<00:12,  4.01s/it] 90%|█████████ | 18/20 [01:32<00:07,  3.95s/it] 95%|█████████▌| 19/20 [01:36<00:03,  3.90s/it]100%|██████████| 20/20 [01:40<00:00,  3.88s/it]100%|██████████| 20/20 [01:40<00:00,  5.00s/it]
Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Map:  50%|█████     | 5041/10000 [00:00<00:00, 50118.25 examples/s]Map: 100%|██████████| 10000/10000 [00:00<00:00, 47616.66 examples/s]
Creating CSV from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 522.20ba/s]
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:45,  5.57s/it] 10%|█         | 2/20 [00:12<01:59,  6.63s/it] 15%|█▌        | 3/20 [00:20<01:58,  6.99s/it] 20%|██        | 4/20 [00:27<01:55,  7.22s/it] 25%|██▌       | 5/20 [00:35<01:48,  7.24s/it] 30%|███       | 6/20 [00:42<01:43,  7.39s/it] 35%|███▌      | 7/20 [00:50<01:35,  7.35s/it] 40%|████      | 8/20 [00:57<01:29,  7.44s/it] 45%|████▌     | 9/20 [01:05<01:22,  7.49s/it] 50%|█████     | 10/20 [01:09<01:05,  6.51s/it] 55%|█████▌    | 11/20 [01:13<00:51,  5.68s/it] 60%|██████    | 12/20 [01:17<00:40,  5.10s/it] 65%|██████▌   | 13/20 [01:21<00:32,  4.71s/it] 70%|███████   | 14/20 [01:24<00:26,  4.42s/it] 75%|███████▌  | 15/20 [01:28<00:21,  4.22s/it] 80%|████████  | 16/20 [01:32<00:16,  4.08s/it] 85%|████████▌ | 17/20 [01:36<00:11,  3.99s/it] 90%|█████████ | 18/20 [01:39<00:07,  3.92s/it] 95%|█████████▌| 19/20 [01:43<00:03,  3.87s/it]100%|██████████| 20/20 [01:47<00:00,  4.01s/it]100%|██████████| 20/20 [01:47<00:00,  5.40s/it]
Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Map:  51%|█████     | 5097/10000 [00:00<00:00, 50785.64 examples/s]Map: 100%|██████████| 10000/10000 [00:00<00:00, 47953.15 examples/s]
Creating CSV from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 551.65ba/s]
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:07<02:23,  7.58s/it] 10%|█         | 2/20 [00:14<02:13,  7.44s/it] 15%|█▌        | 3/20 [00:22<02:05,  7.39s/it] 20%|██        | 4/20 [00:27<01:46,  6.68s/it] 25%|██▌       | 5/20 [00:35<01:43,  6.89s/it] 30%|███       | 6/20 [00:40<01:30,  6.48s/it] 35%|███▌      | 7/20 [00:47<01:26,  6.68s/it] 40%|████      | 8/20 [00:51<01:09,  5.76s/it] 45%|████▌     | 9/20 [00:56<01:01,  5.55s/it] 50%|█████     | 10/20 [01:01<00:53,  5.34s/it] 55%|█████▌    | 11/20 [01:07<00:48,  5.42s/it] 60%|██████    | 12/20 [01:14<00:48,  6.09s/it] 65%|██████▌   | 13/20 [01:18<00:37,  5.42s/it] 70%|███████   | 14/20 [01:22<00:29,  4.92s/it] 75%|███████▌  | 15/20 [01:26<00:22,  4.58s/it] 80%|████████  | 16/20 [01:30<00:17,  4.34s/it] 85%|████████▌ | 17/20 [01:33<00:12,  4.16s/it] 90%|█████████ | 18/20 [01:37<00:08,  4.04s/it] 95%|█████████▌| 19/20 [01:41<00:03,  3.96s/it]100%|██████████| 20/20 [01:45<00:00,  3.91s/it]100%|██████████| 20/20 [01:45<00:00,  5.26s/it]
Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Map:  53%|█████▎    | 5278/10000 [00:00<00:00, 52586.47 examples/s]Map: 100%|██████████| 10000/10000 [00:00<00:00, 49003.40 examples/s]
Creating CSV from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 573.96ba/s]
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:45,  5.54s/it] 10%|█         | 2/20 [00:11<01:40,  5.60s/it] 15%|█▌        | 3/20 [00:18<01:50,  6.52s/it] 20%|██        | 4/20 [00:25<01:45,  6.59s/it] 25%|██▌       | 5/20 [00:32<01:39,  6.63s/it] 30%|███       | 6/20 [00:39<01:36,  6.87s/it] 35%|███▌      | 7/20 [00:46<01:31,  7.00s/it] 40%|████      | 8/20 [00:50<01:11,  5.98s/it] 45%|████▌     | 9/20 [00:58<01:11,  6.49s/it] 50%|█████     | 10/20 [01:03<01:00,  6.06s/it] 55%|█████▌    | 11/20 [01:08<00:52,  5.86s/it] 60%|██████    | 12/20 [01:12<00:42,  5.26s/it] 65%|██████▌   | 13/20 [01:16<00:33,  4.82s/it] 70%|███████   | 14/20 [01:20<00:26,  4.50s/it] 75%|███████▌  | 15/20 [01:24<00:21,  4.36s/it] 80%|████████  | 16/20 [01:27<00:16,  4.18s/it] 85%|████████▌ | 17/20 [01:31<00:12,  4.05s/it] 90%|█████████ | 18/20 [01:35<00:07,  3.97s/it] 95%|█████████▌| 19/20 [01:39<00:03,  3.91s/it]100%|██████████| 20/20 [01:43<00:00,  3.87s/it]100%|██████████| 20/20 [01:43<00:00,  5.15s/it]
Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Map:  50%|█████     | 5000/10000 [00:00<00:00, 49683.30 examples/s]Map: 100%|██████████| 10000/10000 [00:00<00:00, 49388.17 examples/s]Map: 100%|██████████| 10000/10000 [00:00<00:00, 46875.99 examples/s]
Creating CSV from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 560.28ba/s]
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:07<02:13,  7.01s/it] 10%|█         | 2/20 [00:13<01:57,  6.54s/it] 15%|█▌        | 3/20 [00:20<01:57,  6.91s/it] 20%|██        | 4/20 [00:28<01:54,  7.18s/it] 25%|██▌       | 5/20 [00:35<01:48,  7.21s/it] 30%|███       | 6/20 [00:42<01:40,  7.18s/it] 35%|███▌      | 7/20 [00:47<01:24,  6.50s/it] 40%|████      | 8/20 [00:51<01:07,  5.65s/it] 45%|████▌     | 9/20 [00:55<00:55,  5.08s/it] 50%|█████     | 10/20 [00:59<00:47,  4.70s/it] 55%|█████▌    | 11/20 [01:02<00:39,  4.39s/it] 60%|██████    | 12/20 [01:07<00:34,  4.34s/it] 65%|██████▌   | 13/20 [01:10<00:28,  4.14s/it] 70%|███████   | 14/20 [01:14<00:23,  3.99s/it] 75%|███████▌  | 15/20 [01:18<00:19,  3.89s/it] 80%|████████  | 16/20 [01:21<00:15,  3.82s/it] 85%|████████▌ | 17/20 [01:25<00:11,  3.77s/it] 90%|█████████ | 18/20 [01:29<00:07,  3.73s/it] 95%|█████████▌| 19/20 [01:32<00:03,  3.71s/it]100%|██████████| 20/20 [01:36<00:00,  3.70s/it]100%|██████████| 20/20 [01:36<00:00,  4.82s/it]
Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Map:  55%|█████▍    | 5465/10000 [00:00<00:00, 54445.86 examples/s]Map: 100%|██████████| 10000/10000 [00:00<00:00, 49902.13 examples/s]
Creating CSV from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 548.46ba/s]
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:07<02:15,  7.11s/it] 10%|█         | 2/20 [00:12<01:50,  6.16s/it] 15%|█▌        | 3/20 [00:19<01:52,  6.63s/it] 20%|██        | 4/20 [00:26<01:49,  6.82s/it] 25%|██▌       | 5/20 [00:34<01:43,  6.93s/it] 30%|███       | 6/20 [00:41<01:38,  7.00s/it] 35%|███▌      | 7/20 [00:44<01:16,  5.91s/it] 40%|████      | 8/20 [00:48<01:02,  5.20s/it] 45%|████▌     | 9/20 [00:52<00:51,  4.73s/it] 50%|█████     | 10/20 [00:59<00:55,  5.57s/it] 55%|█████▌    | 11/20 [01:06<00:54,  6.06s/it] 60%|██████    | 12/20 [01:13<00:49,  6.16s/it] 65%|██████▌   | 13/20 [01:16<00:37,  5.41s/it] 70%|███████   | 14/20 [01:20<00:29,  4.88s/it] 75%|███████▌  | 15/20 [01:24<00:22,  4.52s/it] 80%|████████  | 16/20 [01:27<00:17,  4.26s/it] 85%|████████▌ | 17/20 [01:31<00:12,  4.08s/it] 90%|█████████ | 18/20 [01:35<00:07,  3.98s/it] 95%|█████████▌| 19/20 [01:38<00:03,  3.89s/it]100%|██████████| 20/20 [01:42<00:00,  3.83s/it]100%|██████████| 20/20 [01:42<00:00,  5.13s/it]
Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Map:  53%|█████▎    | 5307/10000 [00:00<00:00, 52791.39 examples/s]Map: 100%|██████████| 10000/10000 [00:00<00:00, 49286.13 examples/s]
Creating CSV from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 552.96ba/s]
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:40,  5.27s/it] 10%|█         | 2/20 [00:10<01:37,  5.43s/it] 15%|█▌        | 3/20 [00:18<01:46,  6.26s/it] 20%|██        | 4/20 [00:25<01:48,  6.79s/it] 25%|██▌       | 5/20 [00:31<01:35,  6.37s/it] 30%|███       | 6/20 [00:36<01:24,  6.00s/it] 35%|███▌      | 7/20 [00:41<01:14,  5.77s/it] 40%|████      | 8/20 [00:47<01:07,  5.61s/it] 45%|████▌     | 9/20 [00:50<00:55,  5.01s/it] 50%|█████     | 10/20 [00:56<00:50,  5.08s/it] 55%|█████▌    | 11/20 [01:03<00:51,  5.72s/it] 60%|██████    | 12/20 [01:07<00:43,  5.42s/it] 65%|██████▌   | 13/20 [01:15<00:42,  6.05s/it] 70%|███████   | 14/20 [01:19<00:31,  5.32s/it] 75%|███████▌  | 15/20 [01:22<00:24,  4.82s/it] 80%|████████  | 16/20 [01:26<00:17,  4.47s/it] 85%|████████▌ | 17/20 [01:30<00:12,  4.23s/it] 90%|█████████ | 18/20 [01:33<00:08,  4.06s/it] 95%|█████████▌| 19/20 [01:37<00:03,  3.94s/it]100%|██████████| 20/20 [01:41<00:00,  3.87s/it]100%|██████████| 20/20 [01:41<00:00,  5.05s/it]
Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Map:  56%|█████▌    | 5577/10000 [00:00<00:00, 55536.75 examples/s]Map: 100%|██████████| 10000/10000 [00:00<00:00, 49432.68 examples/s]
Creating CSV from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 560.41ba/s]
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:43,  5.43s/it] 10%|█         | 2/20 [00:10<01:38,  5.48s/it] 15%|█▌        | 3/20 [00:18<01:46,  6.28s/it] 20%|██        | 4/20 [00:25<01:49,  6.86s/it] 25%|██▌       | 5/20 [00:31<01:37,  6.47s/it] 30%|███       | 6/20 [00:38<01:33,  6.70s/it] 35%|███▌      | 7/20 [00:46<01:29,  6.88s/it] 40%|████      | 8/20 [00:50<01:14,  6.19s/it] 45%|████▌     | 9/20 [00:57<01:11,  6.50s/it] 50%|█████     | 10/20 [01:03<01:02,  6.27s/it] 55%|█████▌    | 11/20 [01:10<00:58,  6.54s/it] 60%|██████    | 12/20 [01:15<00:48,  6.07s/it] 65%|██████▌   | 13/20 [01:19<00:37,  5.34s/it] 70%|███████   | 14/20 [01:23<00:28,  4.83s/it] 75%|███████▌  | 15/20 [01:26<00:22,  4.48s/it] 80%|████████  | 16/20 [01:32<00:18,  4.71s/it] 85%|████████▌ | 17/20 [01:35<00:13,  4.40s/it] 90%|█████████ | 18/20 [01:39<00:08,  4.18s/it] 95%|█████████▌| 19/20 [01:43<00:04,  4.02s/it]100%|██████████| 20/20 [01:47<00:00,  4.07s/it]100%|██████████| 20/20 [01:47<00:00,  5.37s/it]
Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Map:  53%|█████▎    | 5304/10000 [00:00<00:00, 52826.32 examples/s]Map: 100%|██████████| 10000/10000 [00:00<00:00, 49538.24 examples/s]
Creating CSV from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 574.55ba/s]
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 8388.61it/s]
Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 341.53it/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1000 examples [00:00, 75247.65 examples/s]
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:11<00:11, 11.50s/it]100%|██████████| 2/2 [00:21<00:00, 10.85s/it]100%|██████████| 2/2 [00:21<00:00, 10.94s/it]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 33954.83 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 171.95ba/s]
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:10<00:10, 10.69s/it]100%|██████████| 2/2 [00:21<00:00, 10.51s/it]100%|██████████| 2/2 [00:21<00:00, 10.53s/it]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 32259.91 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 317.32ba/s]
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:10<00:10, 10.72s/it]100%|██████████| 2/2 [00:21<00:00, 10.51s/it]100%|██████████| 2/2 [00:21<00:00, 10.54s/it]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 34353.63 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 367.89ba/s]
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:10<00:10, 10.64s/it]100%|██████████| 2/2 [00:20<00:00, 10.40s/it]100%|██████████| 2/2 [00:20<00:00, 10.44s/it]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 35183.28 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 370.65ba/s]
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:10<00:10, 10.57s/it]100%|██████████| 2/2 [00:20<00:00, 10.39s/it]100%|██████████| 2/2 [00:20<00:00, 10.41s/it]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 33988.67 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 366.51ba/s]
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:10<00:10, 10.30s/it]100%|██████████| 2/2 [00:20<00:00, 10.32s/it]100%|██████████| 2/2 [00:20<00:00, 10.32s/it]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 33494.14 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 367.66ba/s]
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:10<00:10, 10.71s/it]100%|██████████| 2/2 [00:21<00:00, 10.50s/it]100%|██████████| 2/2 [00:21<00:00, 10.53s/it]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 34171.71 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 371.51ba/s]
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:10<00:10, 10.74s/it]100%|██████████| 2/2 [00:21<00:00, 10.53s/it]100%|██████████| 2/2 [00:21<00:00, 10.56s/it]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 34233.07 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 370.36ba/s]
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:10<00:10, 10.71s/it]100%|██████████| 2/2 [00:21<00:00, 10.56s/it]100%|██████████| 2/2 [00:21<00:00, 10.59s/it]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 34594.77 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 388.25ba/s]
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:10<00:10, 10.68s/it]100%|██████████| 2/2 [00:21<00:00, 10.50s/it]100%|██████████| 2/2 [00:21<00:00, 10.53s/it]
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|██████████| 1000/1000 [00:00<00:00, 29503.90 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 359.10ba/s]
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:24<00:24, 24.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:29<00:00, 13.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:29<00:00, 14.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.55s/it]100%|██████████| 1/1 [00:02<00:00,  2.55s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 7316.84 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 315.88ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:01<00:00,  1.74s/it]100%|██████████| 1/1 [00:01<00:00,  1.74s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 8725.23 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 690.19ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:01<00:00,  1.74s/it]100%|██████████| 1/1 [00:01<00:00,  1.74s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 9165.07 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 726.66ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.04s/it]100%|██████████| 1/1 [00:02<00:00,  2.04s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 8791.43 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 663.03ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.19s/it]100%|██████████| 1/1 [00:02<00:00,  2.19s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 6191.40 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 796.19ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:01<00:00,  1.74s/it]100%|██████████| 1/1 [00:01<00:00,  1.74s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 8504.61 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 734.81ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.04s/it]100%|██████████| 1/1 [00:02<00:00,  2.04s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 9139.91 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 729.57ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.20s/it]100%|██████████| 1/1 [00:02<00:00,  2.20s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 8583.98 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 735.84ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:01<00:00,  1.74s/it]100%|██████████| 1/1 [00:01<00:00,  1.74s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 7929.49 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 656.59ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.04s/it]100%|██████████| 1/1 [00:02<00:00,  2.04s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 8556.84 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1022.25ba/s]
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:03<00:00,  3.11s/it]100%|██████████| 1/1 [00:03<00:00,  3.11s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 5886.08 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 290.18ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:04<00:00,  4.08s/it]100%|██████████| 1/1 [00:04<00:00,  4.08s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 7998.60 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 615.45ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.78s/it]100%|██████████| 1/1 [00:02<00:00,  2.78s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 8755.83 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 693.27ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.76s/it]100%|██████████| 1/1 [00:02<00:00,  2.76s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 7038.60 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 985.04ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.78s/it]100%|██████████| 1/1 [00:02<00:00,  2.78s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 8433.13 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 878.94ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.78s/it]100%|██████████| 1/1 [00:02<00:00,  2.78s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 8662.87 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 782.37ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.77s/it]100%|██████████| 1/1 [00:02<00:00,  2.77s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 8014.03 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 715.26ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.77s/it]100%|██████████| 1/1 [00:02<00:00,  2.77s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 9310.33 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 817.44ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:03<00:00,  3.75s/it]100%|██████████| 1/1 [00:03<00:00,  3.75s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 8356.85 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1026.76ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:02<00:00,  2.78s/it]100%|██████████| 1/1 [00:02<00:00,  2.78s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 9709.26 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 789.89ba/s]
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.57s/it]
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:07<02:22,  7.49s/it] 10%|█         | 2/20 [00:18<02:51,  9.52s/it] 15%|█▌        | 3/20 [00:26<02:28,  8.75s/it] 20%|██        | 4/20 [00:35<02:23,  8.95s/it] 25%|██▌       | 5/20 [00:43<02:08,  8.54s/it] 30%|███       | 6/20 [00:51<01:56,  8.31s/it] 35%|███▌      | 7/20 [00:58<01:43,  7.93s/it] 40%|████      | 8/20 [01:06<01:34,  7.90s/it] 45%|████▌     | 9/20 [01:14<01:29,  8.10s/it] 50%|█████     | 10/20 [01:24<01:26,  8.68s/it] 55%|█████▌    | 11/20 [01:31<01:14,  8.22s/it] 60%|██████    | 12/20 [01:39<01:03,  7.96s/it] 65%|██████▌   | 13/20 [01:48<00:57,  8.20s/it] 70%|███████   | 14/20 [01:58<00:54,  9.01s/it] 75%|███████▌  | 15/20 [02:06<00:43,  8.73s/it] 80%|████████  | 16/20 [02:14<00:33,  8.32s/it] 85%|████████▌ | 17/20 [02:23<00:25,  8.45s/it] 90%|█████████ | 18/20 [02:31<00:16,  8.33s/it] 95%|█████████▌| 19/20 [02:39<00:08,  8.46s/it]100%|██████████| 20/20 [02:47<00:00,  8.34s/it]100%|██████████| 20/20 [02:47<00:00,  8.40s/it]
Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Map:  35%|███▍      | 3477/10000 [00:00<00:00, 23583.32 examples/s]Map:  95%|█████████▌| 9523/10000 [00:00<00:00, 41729.90 examples/s]Map: 100%|██████████| 10000/10000 [00:00<00:00, 36636.05 examples/s]
Creating CSV from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 494.49ba/s]
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:07<02:17,  7.25s/it] 10%|█         | 2/20 [00:17<02:38,  8.80s/it] 15%|█▌        | 3/20 [00:24<02:16,  8.04s/it] 20%|██        | 4/20 [00:31<02:02,  7.69s/it] 20%|██        | 4/20 [00:42<02:50, 10.65s/it]
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/t5_cte.py", line 14, in <module>
    run_experiments.run_experiments(model, tokenizer, batch_size, input_file, output_folder, experiment_count)
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiments.py", line 38, in run_experiments
    predictions = _evaluate_model(model, tokenizer, dataset, batch_size)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiments.py", line 25, in _evaluate_model
    generated_texts = _generate_texts(model, tokenizer, encodings)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiments.py", line 15, in _generate_texts
    generated_ids = model.generate(**encodings, max_new_tokens=20, num_beams=5, do_sample=True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/generation/utils.py", line 1712, in generate
    return self.beam_sample(
           ^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/generation/utils.py", line 3415, in beam_sample
    model_kwargs["past_key_values"] = self._reorder_cache(model_kwargs["past_key_values"], beam_idx)
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 1847, in _reorder_cache
    layer_past_state.index_select(0, beam_idx.to(layer_past_state.device)),
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 79.15 GiB total capacity; 72.35 GiB already allocated; 91.25 MiB free; 78.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.80s/it]
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:11<?, ?it/s]
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/t5_cte.py", line 14, in <module>
    run_experiments.run_experiments(model, tokenizer, batch_size, input_file, output_folder, experiment_count)
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiments.py", line 38, in run_experiments
    predictions = _evaluate_model(model, tokenizer, dataset, batch_size)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiments.py", line 25, in _evaluate_model
    generated_texts = _generate_texts(model, tokenizer, encodings)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiments.py", line 15, in _generate_texts
    generated_ids = model.generate(**encodings, max_new_tokens=20, num_beams=5, do_sample=True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/generation/utils.py", line 1712, in generate
    return self.beam_sample(
           ^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/generation/utils.py", line 3338, in beam_sample
    outputs = self(
              ^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 1746, in forward
    decoder_outputs = self.decoder(
                      ^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 1123, in forward
    layer_outputs = layer_module(
                    ^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 695, in forward
    self_attention_outputs = self.layer[0](
                             ^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 602, in forward
    attention_output = self.SelfAttention(
                       ^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 524, in forward
    key_states = project(
                 ^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 508, in project
    hidden_states = torch.cat([past_key_value, hidden_states], dim=2)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 176.00 MiB (GPU 0; 79.15 GiB total capacity; 72.85 GiB already allocated; 49.25 MiB free; 78.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.55s/it]
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:07<?, ?it/s]
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/t5_cte.py", line 14, in <module>
    run_experiments.run_experiments(model, tokenizer, batch_size, input_file, output_folder, experiment_count)
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiments.py", line 38, in run_experiments
    predictions = _evaluate_model(model, tokenizer, dataset, batch_size)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiments.py", line 25, in _evaluate_model
    generated_texts = _generate_texts(model, tokenizer, encodings)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiments.py", line 15, in _generate_texts
    generated_ids = model.generate(**encodings, max_new_tokens=20, num_beams=5, do_sample=True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/generation/utils.py", line 1712, in generate
    return self.beam_sample(
           ^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/generation/utils.py", line 3338, in beam_sample
    outputs = self(
              ^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 1746, in forward
    decoder_outputs = self.decoder(
                      ^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 1123, in forward
    layer_outputs = layer_module(
                    ^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 755, in forward
    hidden_states = self.layer[-1](hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 344, in forward
    forwarded_states = self.DenseReluDense(forwarded_states)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 312, in forward
    hidden_gelu = self.act(self.wi_0(hidden_states))
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/activations.py", line 56, in forward
    return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))
                                                                                          ^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 79.15 GiB total capacity; 77.70 GiB already allocated; 19.25 MiB free; 78.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:32<02:11, 32.92s/it]Loading checkpoint shards:  40%|████      | 2/5 [01:05<01:38, 32.99s/it]Loading checkpoint shards:  60%|██████    | 3/5 [01:40<01:07, 33.66s/it]Loading checkpoint shards:  80%|████████  | 4/5 [02:18<00:35, 35.24s/it]Loading checkpoint shards: 100%|██████████| 5/5 [02:40<00:00, 30.56s/it]Loading checkpoint shards: 100%|██████████| 5/5 [02:40<00:00, 32.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:08<00:00,  8.11s/it]100%|██████████| 1/1 [00:08<00:00,  8.11s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 3625.62 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 17.54ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.13s/it]100%|██████████| 1/1 [00:05<00:00,  5.13s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 9792.45 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 804.12ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.13s/it]100%|██████████| 1/1 [00:05<00:00,  5.13s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 9924.76 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 682.56ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.13s/it]100%|██████████| 1/1 [00:05<00:00,  5.13s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 8122.04 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1021.01ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.13s/it]100%|██████████| 1/1 [00:05<00:00,  5.13s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 8593.30 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 618.45ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.13s/it]100%|██████████| 1/1 [00:05<00:00,  5.13s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 9079.76 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 921.62ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.13s/it]100%|██████████| 1/1 [00:05<00:00,  5.13s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 7930.24 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 979.52ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.13s/it]100%|██████████| 1/1 [00:05<00:00,  5.13s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 9425.40 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 861.43ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.13s/it]100%|██████████| 1/1 [00:05<00:00,  5.13s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 7063.97 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 800.13ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.13s/it]100%|██████████| 1/1 [00:05<00:00,  5.13s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 9257.11 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1065.08ba/s]
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [01:01<04:04, 61.20s/it]Loading checkpoint shards:  40%|████      | 2/5 [01:34<02:15, 45.03s/it]Loading checkpoint shards:  60%|██████    | 3/5 [02:10<01:21, 40.76s/it]Loading checkpoint shards:  80%|████████  | 4/5 [02:31<00:33, 33.05s/it]Loading checkpoint shards: 100%|██████████| 5/5 [02:32<00:00, 21.50s/it]Loading checkpoint shards: 100%|██████████| 5/5 [02:32<00:00, 30.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:08<00:00,  8.33s/it]100%|██████████| 1/1 [00:08<00:00,  8.33s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 2104.85 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 13.38ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.75s/it]100%|██████████| 1/1 [00:05<00:00,  5.75s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 6813.47 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 707.78ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.75s/it]100%|██████████| 1/1 [00:05<00:00,  5.75s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 9061.32 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 615.27ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.75s/it]100%|██████████| 1/1 [00:05<00:00,  5.75s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 5951.65 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 645.77ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.75s/it]100%|██████████| 1/1 [00:05<00:00,  5.75s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 9078.38 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 578.29ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.75s/it]100%|██████████| 1/1 [00:05<00:00,  5.75s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 8404.07 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 926.92ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.75s/it]100%|██████████| 1/1 [00:05<00:00,  5.75s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 8454.21 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 988.06ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.75s/it]100%|██████████| 1/1 [00:05<00:00,  5.75s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 8955.68 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 544.15ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.75s/it]100%|██████████| 1/1 [00:05<00:00,  5.75s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 8895.09 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 668.63ba/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:05<00:00,  5.75s/it]100%|██████████| 1/1 [00:05<00:00,  5.75s/it]
Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 8095.08 examples/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 545.00ba/s]
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:34<02:17, 34.26s/it]Loading checkpoint shards:  40%|████      | 2/5 [01:12<01:49, 36.51s/it]Loading checkpoint shards:  60%|██████    | 3/5 [01:49<01:13, 36.73s/it]Loading checkpoint shards:  80%|████████  | 4/5 [02:30<00:38, 38.32s/it]Loading checkpoint shards: 100%|██████████| 5/5 [02:55<00:00, 33.58s/it]Loading checkpoint shards: 100%|██████████| 5/5 [02:55<00:00, 35.05s/it]
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:17<?, ?it/s]
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/t5_cte.py", line 14, in <module>
    run_experiments.run_experiments(model, tokenizer, batch_size, input_file, output_folder, experiment_count)
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiments.py", line 38, in run_experiments
    predictions = _evaluate_model(model, tokenizer, dataset, batch_size)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiments.py", line 25, in _evaluate_model
    generated_texts = _generate_texts(model, tokenizer, encodings)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiments.py", line 15, in _generate_texts
    generated_ids = model.generate(**encodings, max_new_tokens=20, num_beams=5, do_sample=True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/generation/utils.py", line 1712, in generate
    return self.beam_sample(
           ^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/generation/utils.py", line 3338, in beam_sample
    outputs = self(
              ^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 1746, in forward
    decoder_outputs = self.decoder(
                      ^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 1123, in forward
    layer_outputs = layer_module(
                    ^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 725, in forward
    cross_attention_outputs = self.layer[1](
                              ^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 636, in forward
    attention_output = self.EncDecAttention(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 532, in forward
    scores = torch.matmul(
             ^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 782.00 MiB (GPU 0; 79.15 GiB total capacity; 77.74 GiB already allocated; 711.25 MiB free; 77.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:33<02:14, 33.67s/it]Loading checkpoint shards:  40%|████      | 2/5 [01:09<01:44, 34.76s/it]Loading checkpoint shards:  60%|██████    | 3/5 [01:13<00:41, 20.79s/it]Loading checkpoint shards:  80%|████████  | 4/5 [01:15<00:13, 13.24s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:16<00:00,  8.83s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:16<00:00, 15.21s/it]
  0%|          | 0/20 [00:00<?, ?it/s]  0%|          | 0/20 [00:16<?, ?it/s]
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/t5_cte.py", line 14, in <module>
    run_experiments.run_experiments(model, tokenizer, batch_size, input_file, output_folder, experiment_count)
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiments.py", line 38, in run_experiments
    predictions = _evaluate_model(model, tokenizer, dataset, batch_size)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiments.py", line 25, in _evaluate_model
    generated_texts = _generate_texts(model, tokenizer, encodings)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiments.py", line 15, in _generate_texts
    generated_ids = model.generate(**encodings, max_new_tokens=20, num_beams=5, do_sample=True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/generation/utils.py", line 1712, in generate
    return self.beam_sample(
           ^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/generation/utils.py", line 3338, in beam_sample
    outputs = self(
              ^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 1746, in forward
    decoder_outputs = self.decoder(
                      ^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 1123, in forward
    layer_outputs = layer_module(
                    ^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 725, in forward
    cross_attention_outputs = self.layer[1](
                              ^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 636, in forward
    attention_output = self.EncDecAttention(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 573, in forward
    attn_output = unshape(torch.matmul(attn_weights, value_states))  # (batch_size, seq_length, dim)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 79.15 GiB total capacity; 78.52 GiB already allocated; 13.25 MiB free; 78.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:32<02:09, 32.30s/it]Loading checkpoint shards:  40%|████      | 2/5 [01:06<01:39, 33.28s/it]Loading checkpoint shards:  60%|██████    | 3/5 [01:41<01:08, 34.11s/it]Loading checkpoint shards:  80%|████████  | 4/5 [02:19<00:35, 35.75s/it]Loading checkpoint shards: 100%|██████████| 5/5 [02:42<00:00, 31.04s/it]Loading checkpoint shards: 100%|██████████| 5/5 [02:42<00:00, 32.47s/it]
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:21<?, ?it/s]
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/t5_cte.py", line 14, in <module>
    run_experiments.run_experiments(model, tokenizer, batch_size, input_file, output_folder, experiment_count)
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiments.py", line 38, in run_experiments
    predictions = _evaluate_model(model, tokenizer, dataset, batch_size)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiments.py", line 25, in _evaluate_model
    generated_texts = _generate_texts(model, tokenizer, encodings)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/run_experiments.py", line 15, in _generate_texts
    generated_ids = model.generate(**encodings, max_new_tokens=20, num_beams=5, do_sample=True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/generation/utils.py", line 1712, in generate
    return self.beam_sample(
           ^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/generation/utils.py", line 3338, in beam_sample
    outputs = self(
              ^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 1746, in forward
    decoder_outputs = self.decoder(
                      ^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 1123, in forward
    layer_outputs = layer_module(
                    ^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 725, in forward
    cross_attention_outputs = self.layer[1](
                              ^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 636, in forward
    attention_output = self.EncDecAttention(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 527, in forward
    value_states = project(
                   ^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 502, in project
    hidden_states = shape(proj_layer(key_value_states))
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.26 GiB (GPU 0; 79.15 GiB total capacity; 78.04 GiB already allocated; 455.25 MiB free; 78.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
