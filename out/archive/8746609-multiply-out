mkdir: cannot create directory â€˜/scratch/general/vast/u1283221/huggingface_cacheâ€™: File exists
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:28<01:54, 28.65s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:59<01:29, 29.88s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:31<01:02, 31.11s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:08<00:33, 33.10s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:16<00:00, 24.13s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:16<00:00, 27.27s/it]
>> Input file: ./data/multiplication/instruction_type-1/prompt_type-1/multiply-primed-1.csv, Output file: ./results/multiplication/instruction_type-1/prompt_type-1/multiply-primed-1/google/flan-t5-xxl/experiment-5.csv, Module: google/flan-t5-xxl, Batch size: 20
Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 10407.70it/s]
Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 54.89it/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 10000 examples [00:00, 77720.94 examples/s]Generating train split: 65000 examples [00:00, 309833.57 examples/s]Generating train split: 65000 examples [00:00, 250363.67 examples/s]
<class 'datasets.arrow_dataset.Dataset'>
  0%|          | 0/3250 [00:00<?, ?it/s]
Map:   0%|          | 0/20 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 378.26 examples/s]
  0%|          | 1/3250 [00:20<18:53:01, 20.92s/it]  0%|          | 2/3250 [00:24<9:48:44, 10.88s/it]   0%|          | 3/3250 [00:29<7:21:42,  8.16s/it]  0%|          | 4/3250 [00:33<5:52:58,  6.52s/it]  0%|          | 5/3250 [00:38<5:15:16,  5.83s/it]  0%|          | 6/3250 [00:43<5:11:54,  5.77s/it]  0%|          | 7/3250 [00:46<4:21:28,  4.84s/it]  0%|          | 8/3250 [00:50<4:03:21,  4.50s/it]  0%|          | 9/3250 [00:54<3:47:03,  4.20s/it]  0%|          | 10/3250 [00:57<3:30:50,  3.90s/it]  0%|          | 11/3250 [01:02<3:41:38,  4.11s/it]  0%|          | 12/3250 [01:07<3:59:06,  4.43s/it]  0%|          | 13/3250 [01:11<3:51:47,  4.30s/it]  0%|          | 14/3250 [01:15<3:59:43,  4.44s/it]  0%|          | 15/3250 [01:19<3:46:06,  4.19s/it]  0%|          | 16/3250 [01:23<3:38:19,  4.05s/it]  1%|          | 17/3250 [01:29<4:11:37,  4.67s/it]  1%|          | 18/3250 [01:32<3:41:51,  4.12s/it]  1%|          | 19/3250 [01:35<3:30:39,  3.91s/it]  1%|          | 20/3250 [01:39<3:33:01,  3.96s/it]  1%|          | 21/3250 [01:42<3:20:37,  3.73s/it]  1%|          | 21/3250 [01:49<4:39:35,  5.20s/it]
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/./scripts/t5_cte.py", line 18, in <module>
    experiment.run_experiment(model, tokenizer, batch_size, input_file_path, output_file_path)
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/scripts/experiment.py", line 54, in run_experiment
    predictions = _evaluate_model(model, tokenizer, dataset, batch_size)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/scripts/experiment.py", line 41, in _evaluate_model
    generated_texts = _generate_texts(model, tokenizer, encodings)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/scripts/experiment.py", line 31, in _generate_texts
    generated_ids = model.generate(**encodings, max_new_tokens=20, num_beams=5, do_sample=True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/generation/utils.py", line 1712, in generate
    return self.beam_sample(
           ^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/generation/utils.py", line 3338, in beam_sample
    outputs = self(
              ^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 1774, in forward
    lm_logits = self.lm_head(sequence_output)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/accelerate/hooks.py", line 160, in new_forward
    args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/accelerate/hooks.py", line 286, in pre_forward
    set_module_tensor_to_device(
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/accelerate/utils/modeling.py", line 313, in set_module_tensor_to_device
    new_value = value.to(device)
                ^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 502.00 MiB (GPU 0; 44.55 GiB total capacity; 43.16 GiB already allocated; 196.25 MiB free; 44.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
