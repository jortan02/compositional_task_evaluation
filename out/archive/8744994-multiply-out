mkdir: cannot create directory â€˜/scratch/general/vast/u1283221/huggingface_cacheâ€™: File exists
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 1/5 [00:24<01:38, 24.52s/it]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:49<01:14, 24.79s/it]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:15<00:50, 25.26s/it]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [01:43<00:26, 26.52s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:50<00:00, 19.26s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:50<00:00, 22.03s/it]
>> Input file: ./data/multiplication/instruction_type-1/prompt_type-1/multiply-primed-1.csv, Output file: ./results/multiplication/instruction_type-1/prompt_type-1/multiply-primed-1/google/flan-t5-xxl/experiment-6.csv, Module: google/flan-t5-xxl, Batch size: 25
Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 10255.02it/s]
Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 102.90it/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 10000 examples [00:00, 77471.16 examples/s]Generating train split: 60000 examples [00:00, 273807.42 examples/s]Generating train split: 120000 examples [00:00, 343130.43 examples/s]Generating train split: 180000 examples [00:00, 383903.89 examples/s]Generating train split: 240000 examples [00:00, 410725.85 examples/s]Generating train split: 300000 examples [00:00, 422067.83 examples/s]Generating train split: 325000 examples [00:00, 365338.89 examples/s]
<class 'datasets.arrow_dataset.Dataset'>
  0%|          | 0/13000 [00:00<?, ?it/s]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 960.06 examples/s]
  0%|          | 1/13000 [00:17<61:59:11, 17.17s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 983.13 examples/s]
  0%|          | 2/13000 [00:19<31:03:14,  8.60s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 2117.10 examples/s]
  0%|          | 3/13000 [00:22<21:09:23,  5.86s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 2009.84 examples/s]
  0%|          | 4/13000 [00:24<16:31:02,  4.58s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1843.16 examples/s]
  0%|          | 5/13000 [00:27<13:55:30,  3.86s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1506.60 examples/s]
  0%|          | 6/13000 [00:30<12:17:47,  3.41s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1019.12 examples/s]
  0%|          | 7/13000 [00:32<11:20:16,  3.14s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1529.38 examples/s]
  0%|          | 8/13000 [00:35<10:42:41,  2.97s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1589.33 examples/s]
  0%|          | 9/13000 [00:37<10:16:36,  2.85s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1089.78 examples/s]
  0%|          | 10/13000 [00:40<9:57:09,  2.76s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 2051.16 examples/s]
  0%|          | 11/13000 [00:43<9:45:49,  2.71s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1660.53 examples/s]
  0%|          | 12/13000 [00:45<9:38:29,  2.67s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 2159.34 examples/s]
  0%|          | 13/13000 [00:48<9:32:37,  2.65s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1412.32 examples/s]
  0%|          | 14/13000 [00:50<9:26:37,  2.62s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1643.92 examples/s]
  0%|          | 15/13000 [00:53<9:25:08,  2.61s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1784.45 examples/s]
  0%|          | 16/13000 [00:55<9:24:23,  2.61s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1570.03 examples/s]
  0%|          | 17/13000 [00:58<9:23:10,  2.60s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1370.51 examples/s]
  0%|          | 18/13000 [01:01<9:20:07,  2.59s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1440.23 examples/s]
  0%|          | 19/13000 [01:03<9:20:29,  2.59s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1887.63 examples/s]
  0%|          | 20/13000 [01:06<9:20:52,  2.59s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1000.23 examples/s]
  0%|          | 21/13000 [01:08<9:21:26,  2.60s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1431.19 examples/s]
  0%|          | 22/13000 [01:11<9:18:56,  2.58s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 752.07 examples/s]
  0%|          | 23/13000 [01:14<9:20:47,  2.59s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1813.02 examples/s]
  0%|          | 24/13000 [01:16<9:21:18,  2.60s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1867.95 examples/s]
  0%|          | 25/13000 [01:19<9:21:09,  2.59s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1666.92 examples/s]
  0%|          | 26/13000 [01:21<9:18:30,  2.58s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1707.81 examples/s]
  0%|          | 27/13000 [01:24<9:19:14,  2.59s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1548.83 examples/s]
  0%|          | 28/13000 [01:27<9:50:52,  2.73s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 873.86 examples/s]
  0%|          | 29/13000 [01:30<9:42:58,  2.70s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1427.72 examples/s]
  0%|          | 30/13000 [01:33<10:04:09,  2.79s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1085.83 examples/s]
  0%|          | 31/13000 [01:35<9:51:42,  2.74s/it] 
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1721.94 examples/s]
  0%|          | 32/13000 [01:38<10:13:48,  2.84s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1396.46 examples/s]
  0%|          | 33/13000 [01:41<9:58:06,  2.77s/it] 
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1650.24 examples/s]
  0%|          | 34/13000 [01:43<9:44:32,  2.70s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1825.42 examples/s]
  0%|          | 35/13000 [01:46<9:37:32,  2.67s/it]
Map:   0%|          | 0/25 [00:00<?, ? examples/s][AMap: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1288.75 examples/s]
  0%|          | 35/13000 [01:50<11:20:02,  3.15s/it]
Traceback (most recent call last):
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/./scripts/t5_cte.py", line 18, in <module>
    experiment.run_experiment(model, tokenizer, batch_size, input_file_path, output_file_path)
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/scripts/experiment.py", line 54, in run_experiment
    predictions = _evaluate_model(model, tokenizer, dataset, batch_size)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/scripts/experiment.py", line 41, in _evaluate_model
    generated_texts = _generate_texts(model, tokenizer, encodings)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/compositional_task_evaluation/scripts/experiment.py", line 31, in _generate_texts
    generated_ids = model.generate(**encodings, max_new_tokens=20, num_beams=5, do_sample=True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/generation/utils.py", line 1712, in generate
    return self.beam_sample(
           ^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/generation/utils.py", line 3338, in beam_sample
    outputs = self(
              ^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py", line 1774, in forward
    lm_logits = self.lm_head(sequence_output)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/accelerate/hooks.py", line 160, in new_forward
    args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/accelerate/hooks.py", line 286, in pre_forward
    set_module_tensor_to_device(
  File "/uufs/chpc.utah.edu/common/home/u1283221/miniconda3/envs/compositional/lib/python3.11/site-packages/accelerate/utils/modeling.py", line 313, in set_module_tensor_to_device
    new_value = value.to(device)
                ^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 502.00 MiB (GPU 0; 44.55 GiB total capacity; 43.57 GiB already allocated; 220.25 MiB free; 44.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
